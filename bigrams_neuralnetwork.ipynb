{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int representation of first chars: tensor([ 0,  5, 13, 13,  1])\n",
      "Int representation of second chars: tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "#\n",
    "#      BIGRAM LLM build with a Neural Network\n",
    "#\n",
    "###############################################\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('names.txt', 'r') as file:\n",
    "    names = file.readlines()\n",
    "names = [name.strip().lower() for name in names] # only lowercase letters to get 26 chars\n",
    "\n",
    "SPECIAL_CH = '.'\n",
    "\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)} # {'a':1, 'b':2, 'c':3, ..., 'z':26}\n",
    "# print(stoi)\n",
    "stoi[SPECIAL_CH] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "# include counting the special char with the chars in the dataset - 27 total\n",
    "NUM_CHARS = len(chars + [SPECIAL_CH])\n",
    "\n",
    "# create a training set of bigrams ( x (1st char),y (2nd char) )\n",
    "\n",
    "# inputs (xs) and targets (ys)\n",
    "xs, ys = [], []\n",
    "\n",
    "for name in names[:1]:\n",
    "    chs = [SPECIAL_CH] + list(name) + [SPECIAL_CH]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1] # get the number for the char from the stoi dict\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1) # first chars\n",
    "        ys.append(ix2) # second chars\n",
    "\n",
    "# create tensors from the lists of bigrams assembled\n",
    "xs = torch.tensor(xs) # note: use lowercase tensor() not Tensor() - that one forces the dtype to be float and lowercase tensor() infers the datatype\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "print(f'Int representation of first chars: {xs}') # inputs\n",
    "print(f'Int representation of second chars: {ys}') # labels (targets)\n",
    "# When xs[i] (first chars in bigram) is entered we want ys[i] (second chars in bigram) to have a high probability: example: When 0 is entered we want 5 to have a high probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unencoded Inputs: tensor([ 0,  5, 13, 13,  1])\n",
      "\n",
      "Encoded shape: torch.Size([5, 27])\n",
      "torch.float32\n",
      "Onehot encoded inputs: tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################\n",
    "# ONE HOT ENCODING\n",
    "###################\n",
    "\n",
    "# Not ideal to pass in integers to neural networks (due to calculations on floats), so we use One Hot Encoding\n",
    "# We want float values for a nueral net so they can take on various/continuous values\n",
    "# create a vector made up of dimensions matching the integer and turn the i-th element (the integer index) into a 1\n",
    "# This vector can feed into a neural net\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(f'Unencoded Inputs: {xs}\\n')\n",
    "# one hot encoding. Pass in the integers you want to encode. num_classes is how many elements in the vector\n",
    "xenc = F.one_hot(xs, num_classes=NUM_CHARS) # we only need 27 elements in the vector representing 26 letters of the dataset and 1 special token '.'\n",
    "print(f'Encoded shape: {xenc.shape}') # [5,27] one row for each letter, 27 elements in each vector\n",
    "# print(xenc.dtype) # int64 - caution!\n",
    "\n",
    "# cast the returned type from one_hot() to a float (it returns int64 integers, but we need floats to feed into neural nets)\n",
    "xenc = xenc.float()\n",
    "print(xenc.dtype)\n",
    "print(f'Onehot encoded inputs: {xenc}')\n",
    "\n",
    "plt.imshow(xenc) # visualize the one hot encoded chars\n",
    "plt.show()\n",
    "\n",
    "# each row is an example that can be fed into a neural net. The appropriate bit is turned on as a 1 (yellow block) and everything else is 0 (purple blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[ 1.4486],\n",
      "        [-0.6717],\n",
      "        [-0.3561],\n",
      "        [ 1.5203],\n",
      "        [ 1.4628],\n",
      "        [-1.3405],\n",
      "        [-0.9905],\n",
      "        [ 0.2023],\n",
      "        [-1.2606],\n",
      "        [ 0.4803],\n",
      "        [-0.1919],\n",
      "        [ 0.0522],\n",
      "        [-0.1476],\n",
      "        [-0.3931],\n",
      "        [-0.1939],\n",
      "        [ 0.2726],\n",
      "        [-1.1932],\n",
      "        [-0.5474],\n",
      "        [ 0.7891],\n",
      "        [ 1.4749],\n",
      "        [ 0.8353],\n",
      "        [ 0.7368],\n",
      "        [-1.2107],\n",
      "        [-1.4825],\n",
      "        [-1.7544],\n",
      "        [-1.5090],\n",
      "        [-1.9375]])\n",
      "\n",
      "xenc@W: tensor([[ 1.4486],\n",
      "        [-1.3405],\n",
      "        [-0.3931],\n",
      "        [-0.3931],\n",
      "        [-0.6717]])\n",
      "\n",
      "turned_on_indices=(tensor([0, 1, 2, 3, 4]), tensor([ 0,  5, 13, 13,  1]))\n",
      "\n",
      "First char bit turned on (=1.0): row=0,col=0 = corresponding weight val 1.4486 = row 0 in W\n",
      "Second char bit turned on (=1.0): row=1,col=5 = corresponding weight val -1.3405 = row 5 in W\n"
     ]
    }
   ],
   "source": [
    "########## Feed into Neurons ##############\n",
    "\n",
    "### How matrix multiplication works ###\n",
    "\n",
    "# Define the weights - use random nums\n",
    "W = torch.randn((NUM_CHARS,1)) # normally distributed numbers - most will be around 0, and the tails are thin around magnitude of 3,-3\n",
    "print(f'Weights: {W}\\n') # Column vector of 27 (NUM_CHARS) numbers - these will be multiplied by the inputs\n",
    "\n",
    "# multiply the encoded inputs by the weights using matrix multiplication\n",
    "print(f'xenc@W: {xenc @ W}\\n')\n",
    "\n",
    "# matrix multiplication of [5, 27] @ [27, 1] takes the 27 cols of input bits (per row) and multiplies by the 27 rows of W (one weight copied 27 times to fill out each row) and takes the sum (dot product)\n",
    "  # the col values in each of the 5 rows represent the 27 characters and which character is \"turned on\" - the bit as seen in the above xenc output\n",
    "# this shows us the five activations on this neuron depending on each of the 5 inputs\n",
    "\n",
    "turned_on_indices = torch.where(xenc>0) # first tensor = which row, second tensor = which col\n",
    "print(f'{turned_on_indices=}\\n')\n",
    "\n",
    "# Matrix multiplication goes for all the values in the xenc row, they are multplied by each col value in W in this case (since there is only 1 column each row val goes down the vals element-wise))\n",
    "  # Most of the values per row in xenc are 0 until we find the bit representing the char that is turned on. This will be a 1. and will be multiplied by the corresponding col value in W - \n",
    "  # the sum of the dotproduct will match 1xWcol_val since we only have one col in W\n",
    "# xenc[0][0] x W[0][0]\n",
    "# xenc[0][1] x W[1][0]\n",
    "# xenc[0][2] x W[2][0]\n",
    "# ...\n",
    "\n",
    "r = turned_on_indices[0]\n",
    "c = turned_on_indices[1]\n",
    "print(f'First char bit turned on (={xenc[r[0]][c[0]]}): row={r[0]},col={c[0]} = corresponding weight val {W[c[0]][0]:.4f} = row {c[0]} in W')\n",
    "print(f'Second char bit turned on (={xenc[r[1]][c[1]]}): row={r[1]},col={c[1]} = corresponding weight val {W[c[1]][0]:.4f} = row {c[1]} in W')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firing rate of 14th neuron looking at 4th input (row): -1.1339048147201538\n",
      "\n",
      "xenc 4th row: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "W 14th column: tensor([ 0.2373,  0.4346,  0.6916,  0.2275, -1.0118,  0.0521,  0.5254,  0.4825,\n",
      "         0.2600,  0.0154,  2.0296,  0.5192,  0.6778, -1.1339, -1.1208, -1.2048,\n",
      "         0.2204,  0.3787,  1.3834, -0.1648,  2.1482, -0.2642, -0.5372,  0.9618,\n",
      "        -0.7203, -0.8568, -0.9573])\n",
      "Dot product of 4th input against 13th col of W: -1.1339048147201538\n",
      "\n",
      "Inputs x Weights (27 neurons): tensor([[-1.2193,  0.0690, -1.3106, -1.1039,  1.0090, -0.6649, -0.0892, -1.2055,\n",
      "          0.6764,  1.4185, -1.1270,  1.0709, -0.4315,  0.2373, -0.1926,  1.1851,\n",
      "         -1.4457,  0.6543,  2.1094, -0.0901, -1.0851,  0.5153, -0.8900, -0.9564,\n",
      "         -0.8416,  0.0073,  0.2569],\n",
      "        [ 0.4350,  1.7677, -0.7730, -0.2444, -0.7829, -0.8570, -0.6158,  0.9353,\n",
      "         -0.3553,  1.0593, -1.5281, -0.3893,  1.0605,  0.0521, -0.1816, -3.1357,\n",
      "         -2.0044,  0.5538,  1.9149,  0.3647, -0.5378, -1.9018, -1.3970,  0.7497,\n",
      "         -0.5270, -0.9901,  0.4805],\n",
      "        [-2.3189,  0.0687,  0.0859, -0.4170, -1.2184,  0.2703, -0.5999, -0.2017,\n",
      "         -0.5522, -0.1931, -0.2689,  0.5951, -1.3902, -1.1339,  1.4087, -0.3164,\n",
      "          0.3212, -0.3466, -1.1447,  2.4021, -1.3514,  2.7815,  0.6027, -1.1440,\n",
      "         -0.3877,  0.6124, -0.5108],\n",
      "        [-2.3189,  0.0687,  0.0859, -0.4170, -1.2184,  0.2703, -0.5999, -0.2017,\n",
      "         -0.5522, -0.1931, -0.2689,  0.5951, -1.3902, -1.1339,  1.4087, -0.3164,\n",
      "          0.3212, -0.3466, -1.1447,  2.4021, -1.3514,  2.7815,  0.6027, -1.1440,\n",
      "         -0.3877,  0.6124, -0.5108],\n",
      "        [ 1.4930,  1.4537, -1.6755,  0.2992,  0.1372, -0.4280,  1.8867,  0.9627,\n",
      "          0.3218, -0.5400, -0.1290, -0.0471,  0.3505,  0.4346,  0.8363,  0.6487,\n",
      "         -2.3613, -0.9686, -1.7255, -0.4577, -0.7086, -0.8703,  0.4374, -0.5332,\n",
      "          0.1694, -0.5429,  0.3994]])\n"
     ]
    }
   ],
   "source": [
    "# The above is for one neuron, but we want more neurons which represent one layer in a neural net\n",
    "\n",
    "# weights\n",
    "W = torch.randn(NUM_CHARS, NUM_CHARS) # 27x27 - first arg is the column of weight values to use, the second argument represents the number of neurons\n",
    "# [ [...weights], [...for which neuron] ] - rows = weights, cols = neurons\n",
    "\n",
    "dot_prods = xenc @ W # now we have a 5x27 matrix (the five onehot encoded inputs to the NN multiplied by  )\n",
    "\n",
    "# this shows the dot product of the third input and the 13th column of the W (weights) matrix\n",
    "print(f'Firing rate of 14th neuron looking at 4th input (row): {dot_prods[3,13]}\\n')\n",
    "# the firing rate is the dot product of that intput (the row values of xenc) multiplied by the weights (column values) for that neuron:\n",
    "print(f'xenc 4th row: {xenc[3]}\\n')\n",
    "print(f'W 14th column: {W[:,13]}')\n",
    "# print(f'Dot product of 4th input against 13th col of W: {xenc[3] @ W[:,13]}')\n",
    "print(f'Dot product of 4th input against 13th col of W: {(xenc[3] * W[:,13]).sum()}') # should match firing rate printed above\n",
    "\n",
    "### This matrix multiplication allows us to look at multiple input examples input into a layer of neurons in a neural net (i.e. 27 inputs into a layer of 27 neurons here)\n",
    "## NOTE: This layer is a linear layer (there is no bias or squashing function like tanh applied, just weights x inputs).\n",
    "        # This NN is also going to be just one layer (as simple as possible NN)\n",
    "\n",
    "print(f'\\nInputs x Weights (27 neurons): {dot_prods}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values -3.0 and 5.0:\n",
      "0.049787066876888275\n",
      "148.4131622314453\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########### EXPONENTIATION TO USE FOR GETTING LOG COUNTS OF CHARACTERS ###################\n",
    "\n",
    "\n",
    "'''\n",
    "We need a probability distribution for what the next character could be given a character input.\n",
    "\n",
    "Because integers are not ideal in NN input, we cannot use integer counts per char (as in the original non-neural net example of a bigram - see ./bigramllm.ipynb).\n",
    "We therefore use log counts (i.e. 'logits') - we use exponentiatiation of the weight x input dot product outputs of the neural layer to get the log counts.\n",
    "\n",
    "Exponentation: see https://www.wolframalpha.com/input?i=exp%28x%29\n",
    "\n",
    "If negative number input, you will get e^x which is always positive and below +1\n",
    "If positive number input, you get numbers greater than +1 to positive infinity\n",
    "\n",
    "We can use this exponentiated log count to represent the count per character that we used originally.\n",
    "'''\n",
    "\n",
    "print('exponentiated values -3.0 and 5.0:')\n",
    "t = torch.tensor([-3.0,5.0])\n",
    "print(t.exp()[0].item()) # exponentiation: negative input results in positive number below 1\n",
    "print(t.exp()[1].item()) # exponentiation: positive input results in positive number even higher than the input (and greater than 1)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All counts: tensor([[ 0.2954,  1.0714,  0.2697,  0.3316,  2.7427,  0.5143,  0.9146,  0.2995,\n",
      "          1.9667,  4.1308,  0.3240,  2.9180,  0.6495,  1.2679,  0.8248,  3.2709,\n",
      "          0.2356,  1.9238,  8.2431,  0.9139,  0.3379,  1.6741,  0.4107,  0.3843,\n",
      "          0.4310,  1.0073,  1.2929],\n",
      "        [ 1.5450,  5.8576,  0.4616,  0.7832,  0.4571,  0.4244,  0.5402,  2.5479,\n",
      "          0.7010,  2.8845,  0.2169,  0.6776,  2.8877,  1.0534,  0.8339,  0.0435,\n",
      "          0.1347,  1.7399,  6.7861,  1.4401,  0.5840,  0.1493,  0.2473,  2.1164,\n",
      "          0.5903,  0.3715,  1.6169],\n",
      "        [ 0.0984,  1.0711,  1.0897,  0.6590,  0.2957,  1.3104,  0.5489,  0.8174,\n",
      "          0.5757,  0.8244,  0.7643,  1.8132,  0.2490,  0.3218,  4.0904,  0.7288,\n",
      "          1.3787,  0.7071,  0.3183, 11.0466,  0.2589, 16.1432,  1.8270,  0.3185,\n",
      "          0.6786,  1.8449,  0.6000],\n",
      "        [ 0.0984,  1.0711,  1.0897,  0.6590,  0.2957,  1.3104,  0.5489,  0.8174,\n",
      "          0.5757,  0.8244,  0.7643,  1.8132,  0.2490,  0.3218,  4.0904,  0.7288,\n",
      "          1.3787,  0.7071,  0.3183, 11.0466,  0.2589, 16.1432,  1.8270,  0.3185,\n",
      "          0.6786,  1.8449,  0.6000],\n",
      "        [ 4.4502,  4.2791,  0.1872,  1.3487,  1.1470,  0.6518,  6.5975,  2.6187,\n",
      "          1.3796,  0.5827,  0.8789,  0.9540,  1.4198,  1.5444,  2.3077,  1.9130,\n",
      "          0.0943,  0.3796,  0.1781,  0.6328,  0.4924,  0.4188,  1.5486,  0.5867,\n",
      "          1.1846,  0.5811,  1.4909]])\n",
      "\n",
      "Sum of counts per row: tensor([[38.6466],\n",
      "        [37.6921],\n",
      "        [50.3799],\n",
      "        [50.3799],\n",
      "        [39.8483]])\n",
      "\n",
      "Probabilities (count / row_sum): tensor([[0.0076, 0.0277, 0.0070, 0.0086, 0.0710, 0.0133, 0.0237, 0.0078, 0.0509,\n",
      "         0.1069, 0.0084, 0.0755, 0.0168, 0.0328, 0.0213, 0.0846, 0.0061, 0.0498,\n",
      "         0.2133, 0.0236, 0.0087, 0.0433, 0.0106, 0.0099, 0.0112, 0.0261, 0.0335],\n",
      "        [0.0410, 0.1554, 0.0122, 0.0208, 0.0121, 0.0113, 0.0143, 0.0676, 0.0186,\n",
      "         0.0765, 0.0058, 0.0180, 0.0766, 0.0279, 0.0221, 0.0012, 0.0036, 0.0462,\n",
      "         0.1800, 0.0382, 0.0155, 0.0040, 0.0066, 0.0561, 0.0157, 0.0099, 0.0429],\n",
      "        [0.0020, 0.0213, 0.0216, 0.0131, 0.0059, 0.0260, 0.0109, 0.0162, 0.0114,\n",
      "         0.0164, 0.0152, 0.0360, 0.0049, 0.0064, 0.0812, 0.0145, 0.0274, 0.0140,\n",
      "         0.0063, 0.2193, 0.0051, 0.3204, 0.0363, 0.0063, 0.0135, 0.0366, 0.0119],\n",
      "        [0.0020, 0.0213, 0.0216, 0.0131, 0.0059, 0.0260, 0.0109, 0.0162, 0.0114,\n",
      "         0.0164, 0.0152, 0.0360, 0.0049, 0.0064, 0.0812, 0.0145, 0.0274, 0.0140,\n",
      "         0.0063, 0.2193, 0.0051, 0.3204, 0.0363, 0.0063, 0.0135, 0.0366, 0.0119],\n",
      "        [0.1117, 0.1074, 0.0047, 0.0338, 0.0288, 0.0164, 0.1656, 0.0657, 0.0346,\n",
      "         0.0146, 0.0221, 0.0239, 0.0356, 0.0388, 0.0579, 0.0480, 0.0024, 0.0095,\n",
      "         0.0045, 0.0159, 0.0124, 0.0105, 0.0389, 0.0147, 0.0297, 0.0146, 0.0374]])\n"
     ]
    }
   ],
   "source": [
    "# Get the log counts for the neural net layer:\n",
    "logits = xenc @ W # log-counts (the inputs, which bit is on representing the character in the set, multiplied by the weights - each col is the dot product per neuron in the layer)\n",
    "counts = logits.exp() # exponentiated logits: make any negatives positive (to better represent counts), this is equivalent to and can be interpreted as the counts per char\n",
    "print(f'All counts: {counts}') # all will be positive numbers due to the exponentation of the dot product outputs (if they were negative)\n",
    "# NOTE: all of these are differentiable operations that we can back propagate through (i.e. use gradient descent)\n",
    "\n",
    "s = counts.sum(1, keepdim=True) # get sum per row (1st dimension)\n",
    "print(f'\\nSum of counts per row: {s}')\n",
    "\n",
    "# Probabilities using the count normalized by the sum of counts in the row:\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "print(f'\\nProbabilities (count / row_sum): {probs}') # every row sums to 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input char: .\n",
      "Probabilities of which char comes after char input: \n",
      "\t[0.0076, 0.0277, 0.007, 0.0086, 0.071, 0.0133, 0.0237, 0.0078, 0.0509, 0.1069, 0.0084, 0.0755, 0.0168, 0.0328, 0.0213, 0.0846, 0.0061, 0.0498, 0.2133, 0.0236, 0.0087, 0.0433, 0.0106, 0.0099, 0.0112, 0.0261, 0.0335]\n",
      "\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# we now have 27 numbers for each input (i.e. a single char fed into the neural net)\n",
    "i = 0 # first row representing the input of the first character `.` - xs[0]\n",
    "print(f'Input char: {itos[i]}')\n",
    "print(f'Probabilities of which char comes after char input: \\n\\t{[round(n.item(), 4) for n in probs[i]]}\\n')\n",
    "print(probs[i].sum()) # normalized to probs so they all sum to 1.0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
