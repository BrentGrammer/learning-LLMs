{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int representation of first chars: tensor([ 0,  5, 13, 13,  1])\n",
      "Int representation of second chars: tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "#\n",
    "#      BIGRAM LLM build with a Neural Network\n",
    "#\n",
    "###############################################\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('names.txt', 'r') as file:\n",
    "    names = file.readlines()\n",
    "names = [name.strip().lower() for name in names] # only lowercase letters to get 26 chars\n",
    "\n",
    "SPECIAL_CH = '.'\n",
    "\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)} # {'a':1, 'b':2, 'c':3, ..., 'z':26}\n",
    "# print(stoi)\n",
    "stoi[SPECIAL_CH] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "# include counting the special char with the chars in the dataset - 27 total\n",
    "NUM_CHARS = len(chars + [SPECIAL_CH])\n",
    "\n",
    "# create a training set of bigrams ( x (1st char),y (2nd char) )\n",
    "\n",
    "# inputs (xs) and targets (ys)\n",
    "xs, ys = [], []\n",
    "\n",
    "for name in names[:1]:\n",
    "    chs = [SPECIAL_CH] + list(name) + [SPECIAL_CH]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1] # get the number for the char from the stoi dict\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1) # first chars\n",
    "        ys.append(ix2) # second chars\n",
    "\n",
    "# create tensors from the lists of bigrams assembled\n",
    "xs = torch.tensor(xs) # note: use lowercase tensor() not Tensor() - that one forces the dtype to be float and lowercase tensor() infers the datatype\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "print(f'Int representation of first chars: {xs}') # inputs\n",
    "print(f'Int representation of second chars: {ys}') # labels (targets)\n",
    "# When xs[i] (first chars in bigram) is entered we want ys[i] (second chars in bigram) to have a high probability: example: When 0 is entered we want 5 to have a high probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unencoded Inputs: tensor([ 0,  5, 13, 13,  1])\n",
      "\n",
      "Encoded shape: torch.Size([5, 27])\n",
      "torch.float32\n",
      "Onehot encoded inputs: tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################\n",
    "# ONE HOT ENCODING\n",
    "###################\n",
    "\n",
    "# Not ideal to pass in integers to neural networks (due to calculations on floats), so we use One Hot Encoding\n",
    "# We want float values for a nueral net so they can take on various/continuous values\n",
    "# create a vector made up of dimensions matching the integer and turn the i-th element (the integer index) into a 1\n",
    "# This vector can feed into a neural net\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(f'Unencoded Inputs: {xs}\\n')\n",
    "# one hot encoding. Pass in the integers you want to encode. num_classes is how many elements in the vector\n",
    "xenc = F.one_hot(xs, num_classes=NUM_CHARS) # we only need 27 elements in the vector representing 26 letters of the dataset and 1 special token '.'\n",
    "print(f'Encoded shape: {xenc.shape}') # [5,27] one row for each letter, 27 elements in each vector\n",
    "# print(xenc.dtype) # int64 - caution!\n",
    "\n",
    "# cast the returned type from one_hot() to a float (it returns int64 integers, but we need floats to feed into neural nets)\n",
    "xenc = xenc.float()\n",
    "print(xenc.dtype)\n",
    "print(f'Onehot encoded inputs: {xenc}')\n",
    "\n",
    "plt.imshow(xenc) # visualize the one hot encoded chars\n",
    "plt.show()\n",
    "\n",
    "# each row is an example that can be fed into a neural net. The appropriate bit is turned on as a 1 (yellow block) and everything else is 0 (purple blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[-0.9676],\n",
      "        [-0.2160],\n",
      "        [ 1.0075],\n",
      "        [ 1.0206],\n",
      "        [ 0.3977],\n",
      "        [ 0.5869],\n",
      "        [ 0.3841],\n",
      "        [-0.4217],\n",
      "        [ 0.3161],\n",
      "        [-1.0018],\n",
      "        [ 0.7001],\n",
      "        [-0.6360],\n",
      "        [ 0.3252],\n",
      "        [ 0.9606],\n",
      "        [ 0.5879],\n",
      "        [-2.3827],\n",
      "        [-0.0377],\n",
      "        [ 1.1083],\n",
      "        [ 0.5150],\n",
      "        [-0.3147],\n",
      "        [-0.1392],\n",
      "        [ 0.3116],\n",
      "        [-0.8606],\n",
      "        [-1.5258],\n",
      "        [-0.1092],\n",
      "        [-0.1277],\n",
      "        [-0.2291]])\n",
      "\n",
      "xenc@W: tensor([[-0.9676],\n",
      "        [ 0.5869],\n",
      "        [ 0.9606],\n",
      "        [ 0.9606],\n",
      "        [-0.2160]])\n",
      "\n",
      "turned_on_indices=(tensor([0, 1, 2, 3, 4]), tensor([ 0,  5, 13, 13,  1]))\n",
      "\n",
      "First char bit turned on (=1.0): row=0,col=0 = corresponding weight val -0.9676 = row 0 in W\n",
      "Second char bit turned on (=1.0): row=1,col=5 = corresponding weight val 0.5869 = row 5 in W\n"
     ]
    }
   ],
   "source": [
    "########## Feed Input into Neurons ##############\n",
    "\n",
    "# Generate Weights\n",
    "W = torch.randn((NUM_CHARS,1)) # normally distributed numbers - most will be around 0, and the tails are thin around magnitude of 3,-3\n",
    "print(f'Weights: {W}\\n') # Column vector of 27 (NUM_CHARS) numbers - these will be multiplied by the inputs\n",
    "\n",
    "# multiply the encoded inputs by the weights using matrix multiplication\n",
    "print(f'xenc@W: {xenc @ W}\\n')\n",
    "\n",
    "\n",
    "### How matrix multiplication works ###\n",
    "\n",
    "# matrix multiplication of [5, 27] @ [27, 1] takes the 27 cols of input bits (per row) and multiplies by the 27 rows of W (one weight copied 27 times to fill out each row) and takes the sum (dot product)\n",
    "  # the col values in each of the 5 rows represent the 27 characters and which character is \"turned on\" - the bit as seen in the above xenc output\n",
    "# this shows us the five activations on this neuron depending on each of the 5 inputs\n",
    "\n",
    "turned_on_indices = torch.where(xenc>0) # first tensor = which row, second tensor = which col\n",
    "print(f'{turned_on_indices=}\\n')\n",
    "\n",
    "# Matrix multiplication goes for all the values in the xenc row, they are multplied by each col value in W in this case (since there is only 1 column each row val goes down the vals element-wise))\n",
    "  # Most of the values per row in xenc are 0 until we find the bit representing the char that is turned on. This will be a 1. and will be multiplied by the corresponding col value in W - \n",
    "  # the sum of the dotproduct will match 1xWcol_val since we only have one col in W\n",
    "# xenc[0][0] x W[0][0]\n",
    "# xenc[0][1] x W[1][0]\n",
    "# xenc[0][2] x W[2][0]\n",
    "# ...\n",
    "\n",
    "r = turned_on_indices[0]\n",
    "c = turned_on_indices[1]\n",
    "print(f'First char bit turned on (={xenc[r[0]][c[0]]}): row={r[0]},col={c[0]} = corresponding weight val {W[c[0]][0]:.4f} = row {c[0]} in W')\n",
    "print(f'Second char bit turned on (={xenc[r[1]][c[1]]}): row={r[1]},col={c[1]} = corresponding weight val {W[c[1]][0]:.4f} = row {c[1]} in W')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firing rate of 14th neuron looking at 4th input (row): 0.03788243606686592\n",
      "\n",
      "xenc 4th row: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "W 14th column: tensor([-4.3297e-01, -9.6478e-01,  2.1346e+00, -7.2759e-01, -1.3753e-01,\n",
      "        -1.3132e-03, -3.1566e-01,  1.1199e+00, -4.7701e-01, -1.5065e+00,\n",
      "         1.2108e+00, -4.0636e-01,  1.9110e-01,  3.7882e-02,  1.3431e+00,\n",
      "         2.4311e-01, -1.5364e-01, -1.1583e+00,  1.1614e+00,  4.2016e-01,\n",
      "         2.7359e-01,  3.6583e-01,  1.7128e+00,  1.3769e+00, -8.9778e-01,\n",
      "        -9.5177e-01,  1.4723e-01])\n",
      "Dot product of 4th input against 13th col of W: 0.03788243606686592\n",
      "\n",
      "Inputs x Weights (27 neurons): tensor([[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
      "         -2.9643e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01,\n",
      "         -4.7125e-01,  7.8682e-01, -3.2843e-01, -4.3297e-01,  1.3729e+00,\n",
      "          2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4039e-01,\n",
      "          9.8488e-01, -1.4837e-01, -1.4795e+00,  4.4830e-01, -7.0730e-02,\n",
      "          2.4968e+00,  2.4448e+00],\n",
      "        [ 4.7236e-01,  1.4830e+00,  3.1748e-01,  1.0588e+00,  2.3982e+00,\n",
      "          4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
      "          1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01,\n",
      "          5.5570e-01,  4.7458e-01, -1.3867e+00,  1.6229e+00,  1.7197e-01,\n",
      "          9.8846e-01,  5.0657e-01,  1.0198e+00, -1.9062e+00, -4.2753e-01,\n",
      "         -2.1259e+00,  9.6041e-01],\n",
      "        [ 1.9359e-01,  1.0532e+00,  6.3393e-01,  2.5786e-01,  9.6408e-01,\n",
      "         -2.4855e-01,  2.4756e-02, -3.0404e-02,  1.5622e+00, -4.4852e-01,\n",
      "         -1.2345e+00,  1.1220e+00, -6.7381e-01,  3.7882e-02, -5.5881e-01,\n",
      "         -8.2709e-01,  8.2253e-01, -7.5100e-01,  9.2778e-01, -1.4849e+00,\n",
      "         -2.1293e-01, -1.1860e+00, -6.6092e-01, -2.3348e-01,  1.5447e+00,\n",
      "          6.0061e-01, -7.0909e-01],\n",
      "        [ 1.9359e-01,  1.0532e+00,  6.3393e-01,  2.5786e-01,  9.6408e-01,\n",
      "         -2.4855e-01,  2.4756e-02, -3.0404e-02,  1.5622e+00, -4.4852e-01,\n",
      "         -1.2345e+00,  1.1220e+00, -6.7381e-01,  3.7882e-02, -5.5881e-01,\n",
      "         -8.2709e-01,  8.2253e-01, -7.5100e-01,  9.2778e-01, -1.4849e+00,\n",
      "         -2.1293e-01, -1.1860e+00, -6.6092e-01, -2.3348e-01,  1.5447e+00,\n",
      "          6.0061e-01, -7.0909e-01],\n",
      "        [-6.7006e-01, -1.2199e+00,  3.0314e-01, -1.0725e+00,  7.2762e-01,\n",
      "          5.1114e-02,  1.3095e+00, -8.0220e-01, -8.5042e-01, -1.8068e+00,\n",
      "          1.2523e+00, -1.2256e+00,  1.2165e+00, -9.6478e-01, -2.3211e-01,\n",
      "         -3.4762e-01,  3.3244e-01, -1.3263e+00,  1.1224e+00,  5.9641e-01,\n",
      "          4.5846e-01,  5.4011e-02, -1.7400e+00,  1.1560e-01,  8.0319e-01,\n",
      "          5.4108e-01, -1.1646e+00]])\n"
     ]
    }
   ],
   "source": [
    "# The above is for one neuron, but we want more neurons which represent one layer in a neural net\n",
    "\n",
    "####### GENERATE WEIGHTS FOR NEURONS #######\n",
    "# Define the weights - use random nums - each nueron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((NUM_CHARS, NUM_CHARS), generator=g) # 27x27 - first arg is the column of weight values to use, the second argument represents the number of neurons\n",
    "# [ [...weights], [...for which neuron] ] - rows = weights, cols = neurons\n",
    "\n",
    "dot_prods = xenc @ W # now we have a 5x27 matrix (the five onehot encoded inputs to the NN multiplied by  )\n",
    "\n",
    "# this shows the dot product of the third input and the 13th column of the W (weights) matrix\n",
    "print(f'Firing rate of 14th neuron looking at 4th input (row): {dot_prods[3,13]}\\n')\n",
    "# the firing rate is the dot product of that intput (the row values of xenc) multiplied by the weights (column values) for that neuron:\n",
    "print(f'xenc 4th row: {xenc[3]}\\n')\n",
    "print(f'W 14th column: {W[:,13]}')\n",
    "# print(f'Dot product of 4th input against 13th col of W: {xenc[3] @ W[:,13]}')\n",
    "print(f'Dot product of 4th input against 13th col of W: {(xenc[3] * W[:,13]).sum()}') # should match firing rate printed above\n",
    "\n",
    "### This matrix multiplication allows us to look at multiple input examples input into a layer of neurons in a neural net (i.e. 27 inputs into a layer of 27 neurons here)\n",
    "## NOTE: This layer is a linear layer (there is no bias or squashing function like tanh applied, just weights x inputs).\n",
    "        # This NN is also going to be just one layer (as simple as possible NN)\n",
    "\n",
    "print(f'\\nInputs x Weights (27 neurons): {dot_prods}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values -3.0 and 5.0:\n",
      "0.049787066876888275\n",
      "148.4131622314453\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########### EXPONENTIATION TO USE FOR GETTING LOG COUNTS OF CHARACTERS ###################\n",
    "\n",
    "\n",
    "'''\n",
    "We need a probability distribution for what the next character could be given a character input.\n",
    "\n",
    "Because integers are not ideal in NN input, we cannot use integer counts per char (as in the original non-neural net example of a bigram - see ./bigramllm.ipynb).\n",
    "We therefore use log counts (i.e. 'logits') - we use exponentiatiation of the weight x input dot product outputs of the neural layer to get the log counts.\n",
    "\n",
    "Exponentation: see https://www.wolframalpha.com/input?i=exp%28x%29\n",
    "\n",
    "If negative number input, you will get e^x which is always positive and below +1\n",
    "If positive number input, you get numbers greater than +1 to positive infinity\n",
    "\n",
    "We can use this exponentiated log count to represent the count per character that we used originally.\n",
    "'''\n",
    "\n",
    "print('exponentiated values -3.0 and 5.0:')\n",
    "t = torch.tensor([-3.0,5.0])\n",
    "print(t.exp()[0].item()) # exponentiation: negative input results in positive number below 1\n",
    "print(t.exp()[1].item()) # exponentiation: positive input results in positive number even higher than the input (and greater than 1)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All counts: tensor([[ 4.7940,  0.7888,  0.9730,  0.3326,  1.3309,  0.9708,  0.2129,  1.8311,\n",
      "          1.0824,  2.4710,  0.6242,  2.1964,  0.7200,  0.6486,  3.9469, 18.7908,\n",
      "          4.7673,  0.1967,  1.9683,  0.4315,  2.6775,  0.8621,  0.2277,  1.5656,\n",
      "          0.9317, 12.1434, 11.5281],\n",
      "        [ 1.6038,  4.4060,  1.3737,  2.8830, 11.0032,  1.5972,  0.5187,  1.8527,\n",
      "          0.5369,  1.6654,  3.8818,  1.2642,  0.6339,  0.9987,  0.5995,  1.7432,\n",
      "          1.6073,  0.2499,  5.0680,  1.1876,  2.6871,  1.6596,  2.7728,  0.1486,\n",
      "          0.6521,  0.1193,  2.6128],\n",
      "        [ 1.2136,  2.8669,  1.8850,  1.2942,  2.6224,  0.7799,  1.0251,  0.9701,\n",
      "          4.7691,  0.6386,  0.2910,  3.0710,  0.5098,  1.0386,  0.5719,  0.4373,\n",
      "          2.2763,  0.4719,  2.5289,  0.2265,  0.8082,  0.3054,  0.5164,  0.7918,\n",
      "          4.6866,  1.8232,  0.4921],\n",
      "        [ 1.2136,  2.8669,  1.8850,  1.2942,  2.6224,  0.7799,  1.0251,  0.9701,\n",
      "          4.7691,  0.6386,  0.2910,  3.0710,  0.5098,  1.0386,  0.5719,  0.4373,\n",
      "          2.2763,  0.4719,  2.5289,  0.2265,  0.8082,  0.3054,  0.5164,  0.7918,\n",
      "          4.6866,  1.8232,  0.4921],\n",
      "        [ 0.5117,  0.2953,  1.3541,  0.3422,  2.0701,  1.0524,  3.7043,  0.4483,\n",
      "          0.4272,  0.1642,  3.4984,  0.2936,  3.3753,  0.3811,  0.7929,  0.7064,\n",
      "          1.3944,  0.2655,  3.0723,  1.8156,  1.5816,  1.0555,  0.1755,  1.1225,\n",
      "          2.2327,  1.7179,  0.3120]])\n",
      "\n",
      "Sum of counts per row: tensor([[79.0145],\n",
      "        [55.3268],\n",
      "        [38.9116],\n",
      "        [38.9116],\n",
      "        [34.1630]])\n",
      "\n",
      "Probabilities (count / row_sum): tensor([[0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "         0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "         0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
      "        [0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "         0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "         0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472],\n",
      "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
      "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
      "        [0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "         0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "         0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091]])\n"
     ]
    }
   ],
   "source": [
    "# Get the log counts for the neural net layer:\n",
    "logits = xenc @ W # log-counts (the inputs, which bit is on representing the character in the set, multiplied by the weights - each col is the dot product per neuron in the layer)\n",
    "counts = logits.exp() # exponentiated logits: make any negatives positive (to better represent counts), this is equivalent to and can be interpreted as the counts per char\n",
    "print(f'All counts: {counts}') # all will be positive numbers due to the exponentation of the dot product outputs (if they were negative)\n",
    "# NOTE: all of these are differentiable operations that we can back propagate through (i.e. use gradient descent)\n",
    "\n",
    "s = counts.sum(1, keepdim=True) # get sum per row (1st dimension)\n",
    "print(f'\\nSum of counts per row: {s}')\n",
    "\n",
    "# Probabilities using the count normalized by the sum of counts in the row:\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "print(f'\\nProbabilities (count / row_sum): {probs}') # every row sums to 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input char: .\n",
      "Probabilities of which char comes after char input: \n",
      "\t[0.0607, 0.01, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137, 0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.05, 0.2378, 0.0603, 0.0025, 0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459]\n",
      "\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# we now have 27 numbers for each input (i.e. a single char fed into the neural net)\n",
    "i = 0 # first row representing the input of the first character `.` - xs[0]\n",
    "print(f'Input char: {itos[i]}')\n",
    "print(f'Probabilities of which char comes after char input: \\n\\t{[round(n.item(), 4) for n in probs[i]]}\\n')\n",
    "print(probs[i].sum()) # normalized to probs so they all sum to 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([ 0,  5, 13, 13,  1])\n",
      "Targets (Labels): tensor([ 5, 13, 13,  1,  0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#\n",
    "#      PUTTING IT ALL TOGETHER\n",
    "#\n",
    "###################################\n",
    "\n",
    "# Example training set, i.e. '.emma.'\n",
    "print(f'Inputs: {xs}')\n",
    "print(f'Targets (Labels): {ys}\\n')\n",
    "# first input is the character and the labels are what come after it in this example\n",
    "#  i.e. 'emma' is '.e em mm ma a.' \n",
    "  # if we feed in '.' we expect to get 'e' as the next char from the NN\n",
    "\n",
    "####### GENERATE WEIGHTS FOR NEURONS #######\n",
    "# Define the weights - use random nums - each nueron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((NUM_CHARS, NUM_CHARS), generator=g, requires_grad=True) # we need to tell pytorch we are interested in calculating the gradient for the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## FORWARD PASS ############\n",
    "\n",
    "# NOTE: these are all differentiable operations that we can back propagate through\n",
    "  # Addition, Multiplication, Division, Summing, Exponentiation\n",
    "\n",
    "\n",
    "### Architecture of this Neural Net: 1 Layer followed by soft-max activation function\n",
    "\n",
    "# One-hot encode the inputs\n",
    "xenc = F.one_hot(xs, num_classes=NUM_CHARS).float()\n",
    "logits = xenc @ W # log-counts\n",
    "\n",
    "# these two operations represent 'soft-max'\n",
    "  # soft-max is a activation function used to normalize and convert to outputting probability distributions (all vals sum to 1.0)\n",
    "  # This is something you can put on top of any layer in a NN and it makes the layer output probabilities\n",
    "counts = logits.exp() # e^{log count} - forces to be positive etc.\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next char - normalize with division as part of the soft-max activation function\n",
    "# 5x27 matrix - vector of probabilities that sum to 1.0 for each example input\n",
    "\n",
    "### LOSS ###\n",
    "\n",
    "# For classification as we're doing here (instead of regression as in the basic example in NeuralNetworks/neural_network.ipynb), use negative log likelihood for the loss calculation instead of means squared error\n",
    "\n",
    "# get the probabilities we want to update from the output which are assigned by the NN currently to the next probable characters given the input\n",
    "next_char_probs = probs[torch.arange(5), ys] # for each example input row pluck the probability we're interested in of the next expected char\n",
    "# get the log probability and average down with the mean\n",
    "loss = -next_char_probs.log().mean() # this is the negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ BACKWARD PASS #############\n",
    "\n",
    "# set the gradient to 0 before backward pass\n",
    "W.grad = None # more efficient way in pytorch to zero out the gradient\n",
    "\n",
    "# use torches .backward() to back propagate the loss to fill in the gradients all the way back from the Loss through the operations to the Weights W tensor\n",
    "loss.backward() # make sure you included requires_grad=True on your Weights tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient matrix and Weights matrix have the same shape - every element is telling us what the gradient of the weight is and how it affects the loss\n",
    "# We can now use the gradients to adjust the weights in the opposite direction to lower the loss\n",
    "# print(W.grad) # shows gradients for each weight to reduce loss\n",
    "# W.shape\n",
    "# W.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### UPDATE THE WEIGHTS AFTER THE BACKWARD PASS TO LOWER LOSS ######\n",
    "\n",
    "W.data += -0.1 * W.grad # updates all the weights against the gradient from back propagation\n",
    "# We now expect the loss to decrease after doing another forward pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
