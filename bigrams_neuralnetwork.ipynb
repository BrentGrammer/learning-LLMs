{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int representation of first chars: tensor([ 0,  5, 13, 13,  1])\n",
      "Int representation of second chars: tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "#\n",
    "#      BIGRAM LLM build with a Neural Network\n",
    "#\n",
    "###############################################\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('names.txt', 'r') as file:\n",
    "    names = file.readlines()\n",
    "names = [name.strip().lower() for name in names] # only lowercase letters to get 26 chars\n",
    "\n",
    "SPECIAL_CH = '.'\n",
    "\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)} # {'a':1, 'b':2, 'c':3, ..., 'z':26}\n",
    "# print(stoi)\n",
    "stoi[SPECIAL_CH] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "# include counting the special char with the chars in the dataset - 27 total\n",
    "NUM_CHARS = len(chars + [SPECIAL_CH])\n",
    "\n",
    "# create a training set of bigrams ( x (1st char),y (2nd char) )\n",
    "\n",
    "# inputs (xs) and targets (ys)\n",
    "xs, ys = [], []\n",
    "\n",
    "for name in names[:1]:\n",
    "    chs = [SPECIAL_CH] + list(name) + [SPECIAL_CH]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1] # get the number for the char from the stoi dict\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1) # first chars\n",
    "        ys.append(ix2) # second chars\n",
    "\n",
    "# create tensors from the lists of bigrams assembled\n",
    "xs = torch.tensor(xs) # note: use lowercase tensor() not Tensor() - that one forces the dtype to be float and lowercase tensor() infers the datatype\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "print(f'Int representation of first chars: {xs}') # inputs\n",
    "print(f'Int representation of second chars: {ys}') # labels (targets)\n",
    "# When xs[i] (first chars in bigram) is entered we want ys[i] (second chars in bigram) to have a high probability: example: When 0 is entered we want 5 to have a high probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unencoded Inputs: tensor([ 0,  5, 13, 13,  1])\n",
      "\n",
      "Encoded shape: torch.Size([5, 27])\n",
      "torch.float32\n",
      "\n",
      "Onehot encoded inputs (xenc): tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################\n",
    "# ONE HOT ENCODING\n",
    "###################\n",
    "\n",
    "# Not ideal to pass in integers to neural networks (due to calculations on floats), so we use One Hot Encoding\n",
    "# We want float values for a nueral net so they can take on various/continuous values\n",
    "# create a vector made up of dimensions matching the integer and turn the i-th element (the integer index) into a 1\n",
    "# This vector can feed into a neural net\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(f'Unencoded Inputs: {xs}\\n')\n",
    "# one hot encoding. Pass in the integers you want to encode. num_classes is how many elements in the vector\n",
    "xenc = F.one_hot(xs, num_classes=NUM_CHARS) # we only need 27 elements in the vector representing 26 letters of the dataset and 1 special token '.'\n",
    "print(f'Encoded shape: {xenc.shape}') # [5,27] one row for each letter, 27 elements in each vector\n",
    "# print(xenc.dtype) # int64 - caution!\n",
    "\n",
    "# cast the returned type from one_hot() to a float (it returns int64 integers, but we need floats to feed into neural nets)\n",
    "xenc = xenc.float()\n",
    "print(xenc.dtype)\n",
    "print(f'\\nOnehot encoded inputs (xenc): {xenc}')\n",
    "\n",
    "plt.imshow(xenc) # visualize the one hot encoded chars\n",
    "plt.show()\n",
    "\n",
    "# each row is an example that can be fed into a neural net. The appropriate bit is turned on as a 1 (yellow block) and everything else is 0 (purple blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[ 1.2485],\n",
      "        [ 2.0891],\n",
      "        [ 1.3456],\n",
      "        [-0.4979],\n",
      "        [-0.6950],\n",
      "        [-1.6033],\n",
      "        [ 0.5910],\n",
      "        [ 0.6191],\n",
      "        [-0.2044],\n",
      "        [-1.7460],\n",
      "        [-1.0895],\n",
      "        [-0.3859],\n",
      "        [ 0.4117],\n",
      "        [ 1.1640],\n",
      "        [-1.0882],\n",
      "        [ 1.1278],\n",
      "        [-1.1210],\n",
      "        [ 1.0186],\n",
      "        [-0.8420],\n",
      "        [-1.0710],\n",
      "        [-0.1950],\n",
      "        [-0.5454],\n",
      "        [ 1.3963],\n",
      "        [ 0.5151],\n",
      "        [ 0.0240],\n",
      "        [-0.6053],\n",
      "        [-0.5335]])\n",
      "\n",
      "xenc@W: tensor([[ 1.2485],\n",
      "        [-1.6033],\n",
      "        [ 1.1640],\n",
      "        [ 1.1640],\n",
      "        [ 2.0891]])\n",
      "\n",
      "turned_on_indices=(tensor([0, 1, 2, 3, 4]), tensor([ 0,  5, 13, 13,  1]))\n",
      "\n",
      "First char bit turned on (=1.0): row=0,col=0 = corresponding weight val 1.2485 = row 0 in W\n",
      "Second char bit turned on (=1.0): row=1,col=5 = corresponding weight val -1.6033 = row 5 in W\n"
     ]
    }
   ],
   "source": [
    "########## Feed Input into Neurons ##############\n",
    "\n",
    "# Generate Weights\n",
    "W = torch.randn((NUM_CHARS,1)) # normally distributed numbers - most will be around 0, and the tails are thin around magnitude of 3,-3\n",
    "print(f'Weights: {W}\\n') # Column vector of 27 (NUM_CHARS) numbers - these will be multiplied by the inputs\n",
    "\n",
    "# multiply the encoded inputs by the weights using matrix multiplication\n",
    "print(f'xenc@W: {xenc @ W}\\n')\n",
    "\n",
    "\n",
    "### How matrix multiplication works ###\n",
    "\n",
    "# matrix multiplication of [5, 27] @ [27, 1] takes the 27 cols of input bits (per row) and multiplies by the 27 rows of W (one weight copied 27 times to fill out each row) and takes the sum (dot product)\n",
    "  # the col values in each of the 5 rows represent the 27 characters and which character is \"turned on\" - the bit as seen in the above xenc output\n",
    "# this shows us the five activations on this neuron depending on each of the 5 inputs\n",
    "\n",
    "turned_on_indices = torch.where(xenc>0) # first tensor = which row, second tensor = which col\n",
    "print(f'{turned_on_indices=}\\n')\n",
    "\n",
    "# Matrix multiplication goes for all the values in the xenc row, they are multplied by each col value in W in this case (since there is only 1 column each row val goes down the vals element-wise))\n",
    "  # Most of the values per row in xenc are 0 until we find the bit representing the char that is turned on. This will be a 1. and will be multiplied by the corresponding col value in W - \n",
    "  # the sum of the dotproduct will match 1xWcol_val since we only have one col in W\n",
    "# xenc[0][0] x W[0][0]\n",
    "# xenc[0][1] x W[1][0]\n",
    "# xenc[0][2] x W[2][0]\n",
    "# ...\n",
    "\n",
    "r = turned_on_indices[0]\n",
    "c = turned_on_indices[1]\n",
    "print(f'First char bit turned on (={xenc[r[0]][c[0]]}): row={r[0]},col={c[0]} = corresponding weight val {W[c[0]][0]:.4f} = row {c[0]} in W')\n",
    "print(f'Second char bit turned on (={xenc[r[1]][c[1]]}): row={r[1]},col={c[1]} = corresponding weight val {W[c[1]][0]:.4f} = row {c[1]} in W')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dot_prods' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# print(W[:,2])\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# dot_prods = xenc @ W # now we have a 5x27 matrix (the five onehot encoded inputs to the NN multiplied by  )\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# this shows the dot product of the third input and the 13th column of the W (weights) matrix\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFiring rate of 14th neuron looking at 4th input (row): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdot_prods[\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m13\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# the firing rate is the dot product of that intput (the row values of xenc) multiplied by the weights (column values) for that neuron:\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxenc 4th row: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxenc[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dot_prods' is not defined"
     ]
    }
   ],
   "source": [
    "# The above is for one neuron, but we want more neurons which represent one layer in a neural net\n",
    "\n",
    "####### GENERATE WEIGHTS FOR NEURONS #######\n",
    "# Define the weights - use random nums - each nueron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# W is random normal distribution between -3 and 1\n",
    "W = torch.randn((NUM_CHARS, NUM_CHARS), generator=g) # 27x27 - first arg is the column of weight values to use, the second argument represents the number of neurons\n",
    "# [ [...weights], [...for which neuron] ] - rows = weights, cols = neurons\n",
    "# print('W')\n",
    "# print(W[:,2])\n",
    "\n",
    "dot_prods = xenc @ W # now we have a 5x27 matrix (the five onehot encoded inputs to the NN multiplied by  )\n",
    "\n",
    "# this shows the dot product of the third input and the 13th column of the W (weights) matrix\n",
    "print(f'Firing rate of 14th neuron looking at 4th input (row): {dot_prods[3,13]}\\n')\n",
    "# the firing rate is the dot product of that intput (the row values of xenc) multiplied by the weights (column values) for that neuron:\n",
    "print(f'xenc 4th row: {xenc[3]}\\n')\n",
    "print(f'W 14th column: {W[:,13]}')\n",
    "print(f'\\nRegular Product of 4th col of xenc ({xenc[3,13]}) and 14th col of W ({W[:,13][13]}): {(xenc[3] * W[:,13])}')\n",
    "print(f'\\n4th col of xenc multiplied by 14th col of W with pytorch @ operator: {xenc[3] @ W[:,13]}')\n",
    "print(f'Manual calcd Dot product of 4th input against 14th col of W: {(xenc[3] * W[:,13]).sum()}') # should match firing rate printed above\n",
    "\n",
    "### This matrix multiplication allows us to look at multiple input examples input into a layer of neurons in a neural net (i.e. 27 inputs into a layer of 27 neurons here)\n",
    "## NOTE: This layer is a linear layer (there is no bias or squashing function like tanh applied, just weights x inputs).\n",
    "        # This NN is also going to be just one layer (as simple as possible NN)\n",
    "\n",
    "print(f'\\nInputs x Weights (27 neurons): {dot_prods}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values -3.0 and 5.0:\n",
      "0.049787066876888275\n",
      "148.4131622314453\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########### EXPONENTIATION TO USE FOR GETTING LOG COUNTS OF CHARACTERS ###################\n",
    "\n",
    "\n",
    "'''\n",
    "We need a probability distribution for what the next character could be given a character input.\n",
    "\n",
    "Because integers are not ideal in NN input, we cannot use integer counts per char (as in the original non-neural net example of a bigram - see ./bigramllm.ipynb).\n",
    "We therefore use log counts (i.e. 'logits') - we use exponentiatiation of the weight x input dot product outputs of the neural layer to get the log counts.\n",
    "\n",
    "Exponentation: see https://www.wolframalpha.com/input?i=exp%28x%29\n",
    "\n",
    "If negative number input, you will get e^x which is always positive and below +1\n",
    "If positive number input, you get numbers greater than +1 to positive infinity\n",
    "\n",
    "We can use this exponentiated log count to represent the count per character that we used originally.\n",
    "'''\n",
    "\n",
    "print('exponentiated values -3.0 and 5.0:')\n",
    "t = torch.tensor([-3.0,5.0])\n",
    "print(t.exp()[0].item()) # exponentiation: negative input results in positive number below 1\n",
    "print(t.exp()[1].item()) # exponentiation: positive input results in positive number even higher than the input (and greater than 1)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All counts: tensor([[ 4.7940,  0.7888,  0.9730,  0.3326,  1.3309,  0.9708,  0.2129,  1.8311,\n",
      "          1.0824,  2.4710,  0.6242,  2.1964,  0.7200,  0.6486,  3.9469, 18.7908,\n",
      "          4.7673,  0.1967,  1.9683,  0.4315,  2.6775,  0.8621,  0.2277,  1.5656,\n",
      "          0.9317, 12.1434, 11.5281],\n",
      "        [ 1.6038,  4.4060,  1.3737,  2.8830, 11.0032,  1.5972,  0.5187,  1.8527,\n",
      "          0.5369,  1.6654,  3.8818,  1.2642,  0.6339,  0.9987,  0.5995,  1.7432,\n",
      "          1.6073,  0.2499,  5.0680,  1.1876,  2.6871,  1.6596,  2.7728,  0.1486,\n",
      "          0.6521,  0.1193,  2.6128],\n",
      "        [ 1.2136,  2.8669,  1.8850,  1.2942,  2.6224,  0.7799,  1.0251,  0.9701,\n",
      "          4.7691,  0.6386,  0.2910,  3.0710,  0.5098,  1.0386,  0.5719,  0.4373,\n",
      "          2.2763,  0.4719,  2.5289,  0.2265,  0.8082,  0.3054,  0.5164,  0.7918,\n",
      "          4.6866,  1.8232,  0.4921],\n",
      "        [ 1.2136,  2.8669,  1.8850,  1.2942,  2.6224,  0.7799,  1.0251,  0.9701,\n",
      "          4.7691,  0.6386,  0.2910,  3.0710,  0.5098,  1.0386,  0.5719,  0.4373,\n",
      "          2.2763,  0.4719,  2.5289,  0.2265,  0.8082,  0.3054,  0.5164,  0.7918,\n",
      "          4.6866,  1.8232,  0.4921],\n",
      "        [ 0.5117,  0.2953,  1.3541,  0.3422,  2.0701,  1.0524,  3.7043,  0.4483,\n",
      "          0.4272,  0.1642,  3.4984,  0.2936,  3.3753,  0.3811,  0.7929,  0.7064,\n",
      "          1.3944,  0.2655,  3.0723,  1.8156,  1.5816,  1.0555,  0.1755,  1.1225,\n",
      "          2.2327,  1.7179,  0.3120]])\n",
      "\n",
      "Sum of counts per row: tensor([[79.0145],\n",
      "        [55.3268],\n",
      "        [38.9116],\n",
      "        [38.9116],\n",
      "        [34.1630]])\n",
      "\n",
      "Probabilities (count / row_sum): tensor([[0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "         0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "         0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
      "        [0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "         0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "         0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472],\n",
      "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
      "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
      "        [0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "         0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "         0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091]])\n"
     ]
    }
   ],
   "source": [
    "# Get the log counts for the neural net layer:\n",
    "logits = xenc @ W # log-counts (the inputs, which bit is on representing the character in the set, multiplied by the weights - each col is the dot product per neuron in the layer)\n",
    "counts = logits.exp() # exponentiated logits: make any negatives positive (to better represent counts), this is equivalent to and can be interpreted as the counts per char\n",
    "print(f'All counts: {counts}') # all will be positive numbers due to the exponentation of the dot product outputs (if they were negative)\n",
    "# NOTE: all of these are differentiable operations that we can back propagate through (i.e. use gradient descent)\n",
    "\n",
    "s = counts.sum(1, keepdim=True) # get sum per row (1st dimension)\n",
    "print(f'\\nSum of counts per row: {s}')\n",
    "\n",
    "# Probabilities using the count normalized by the sum of counts in the row:\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "print(f'\\nProbabilities (count / row_sum): {probs}') # every row sums to 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input char: .\n",
      "Probabilities of which char comes after char input: \n",
      "\t[0.0607, 0.01, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137, 0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.05, 0.2378, 0.0603, 0.0025, 0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459]\n",
      "\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# we now have 27 numbers for each input (i.e. a single char fed into the neural net)\n",
    "i = 0 # first row representing the input of the first character `.` - xs[0]\n",
    "print(f'Input char: {itos[i]}')\n",
    "print(f'Probabilities of which char comes after char input: \\n\\t{[round(n.item(), 4) for n in probs[i]]}\\n')\n",
    "print(probs[i].sum()) # normalized to probs so they all sum to 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([ 0,  5, 13, 13,  1])\n",
      "Targets (Labels): tensor([ 5, 13, 13,  1,  0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "#\n",
    "#        PUTTING IT ALL TOGETHER\n",
    "# Using a Nueral Network to predict Bigrams\n",
    "#\n",
    "############################################\n",
    "\n",
    "# Example training set, i.e. '.emma.'\n",
    "print(f'Inputs: {xs}')\n",
    "print(f'Targets (Labels): {ys}\\n')\n",
    "# first input is the character and the labels are what come after it in this example\n",
    "#  i.e. 'emma' is '.e em mm ma a.' \n",
    "  # if we feed in '.' we expect to get 'e' as the next char from the NN\n",
    "\n",
    "#########################\n",
    "#   INITIALIZE WEIGHTS\n",
    "#########################\n",
    "# Define the weights - use random nums - each nueron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# Start with a RANDOM count for all of the characters! (Loss is then guided down via gradient descent from this random starting point)\n",
    "W = torch.randn((NUM_CHARS, NUM_CHARS), generator=g, requires_grad=True) # we need to tell pytorch we are interested in calculating the gradient for the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#      FORWARD PASS \n",
    "#########################\n",
    "\n",
    "# NOTE: these are all differentiable operations that we can back propagate through\n",
    "  # Addition, Multiplication, Division, Summing, Exponentiation\n",
    "\n",
    "\n",
    "### Architecture of this Neural Net: 1 Layer followed by soft-max activation function\n",
    "\n",
    "# One-hot encode the inputs\n",
    "xenc = F.one_hot(xs, num_classes=NUM_CHARS).float()\n",
    "logits = xenc @ W # log-counts\n",
    "\n",
    "# these two operations represent 'soft-max'\n",
    "  # soft-max is a activation function used to normalize and convert to outputting probability distributions (all vals sum to 1.0)\n",
    "  # This is something you can put on top of any layer in a NN and it makes the layer output probabilities\n",
    "counts = logits.exp() # e^{log count} - forces to be positive etc.\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next char - normalize with division as part of the soft-max activation function\n",
    "# 5x27 matrix - vector of probabilities that sum to 1.0 for each example input\n",
    "\n",
    "### CALCULATE LOSS ###\n",
    "# For classification as we're doing here (instead of regression as in the basic example in NeuralNetworks/neural_network.ipynb), use negative log likelihood for the loss calculation instead of means squared error\n",
    "# get the probabilities we want to update from the output which are assigned by the NN currently to the next probable characters given the input\n",
    "next_char_probs = probs[torch.arange(5), ys] # for each example input row pluck the probability we're interested in of the next expected char\n",
    "# get the log probability and average down with the mean\n",
    "loss = -next_char_probs.log().mean() # this is the negative log likelihood\n",
    "\n",
    "#########################\n",
    "#      BACKWARD PASS \n",
    "#########################\n",
    "\n",
    "# set the gradient to 0 before backward pass\n",
    "W.grad = None # more efficient way in pytorch to zero out the gradient\n",
    "\n",
    "# use torches .backward() to back propagate the loss to fill in the gradients all the way back from the Loss through the operations to the Weights W tensor\n",
    "loss.backward() # make sure you included requires_grad=True on your Weights tensor\n",
    "print(f'Loss: {loss.item()}')\n",
    "\n",
    "# Gradient matrix and Weights matrix have the same shape - every element is telling us what the gradient of the weight is and how it affects the loss\n",
    "# We can now use the gradients to adjust the weights in the opposite direction to lower the loss\n",
    "# print(W.grad) # shows gradients for each weight to reduce loss\n",
    "# W.shape\n",
    "# W.grad.shape\n",
    "\n",
    "#########################\n",
    "#      UPDATE WEIGHTS \n",
    "#########################\n",
    "\n",
    "W.data += -0.1 * W.grad # updates all the weights against the gradient from back propagation\n",
    "# We now expect the loss to decrease after doing another forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important point: \n",
    "# The bigram matrix is initialized randomly to begin with\n",
    "# The loss guides the counts to be filled in via gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704.4127197265625\n",
      "0.9662725925445557\n"
     ]
    }
   ],
   "source": [
    "######## REGULARIZATION ############\n",
    "\n",
    "# Take W char counts matrix and square the values to eliminate signs and take the sum\n",
    "# You get 0 loss if W is 0, and accumulate loss if W has non-zero numbers in it\n",
    "# see timestamp 1:50:49 in https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2\n",
    "# and timestamp 1:51:17 in https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=3\n",
    "# Explanation of regularization at timestamp 1:52:25 in https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=3\n",
    "\n",
    "# This is equivalent to smoothing, but using in the context of Gradient Descent approach.\n",
    "# Getting all the counts initialized in W to 0 (this makes them all 1 when exponentiated), makes the distribution uniform (all counts have equal probability)\n",
    "# The idea is to get the distribution to be uniform\n",
    "\n",
    "print((W**2).sum().item())\n",
    "# You can use mean as well if the sum gives you numbers that are too large\n",
    "print((W**2).mean().item())\n",
    "\n",
    "\n",
    "# You can then set the strength of this regularization and add it to the loss function as an extra optimization component.\n",
    "0.01*(W**2).mean()\n",
    "# It acts as a \"spring force\" to push the values of W towards 0 (to get a uniform distribution) while the loss is trying to make the probabilities work out (?)\n",
    "  # If Ws are 0, you achieve 0 loss\n",
    "  # If Ws are not 0, then you feel a loss (accumulates loss due to W**2 sum/mean above)\n",
    "  # Ws want to be 0 to be uniform (no loss), but also match up to the probabilities of the data (??)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This is equivalent to the Smoothing done in bigramllm.ipynb\n",
    "# The regularization strength is the same as how many counts you add (i.e. +1 or higher to get more uniform distribution) in the example below\n",
    "  # If the regularization strength is too high, then everything will become uniform predictions\n",
    "\n",
    "\n",
    "##################\n",
    "#    Smoothing From bigramllm.ipynb\n",
    "##################\n",
    "\n",
    "# convert all values to float as we used to do for the loop\n",
    "# P = (N+1).float() # add 1 or higher value to all values in the dataset - removes any 0s from the dataset\n",
    "# This removes 0 values (makes them 1 or higher) and is a nicety when measuring the likelihood since it will prevent values with 0 probability resulting showing infinity in the loss function (i.e. negative log likelihood)\n",
    "# The higher the value you add, the more smooth your model will be, the lower the value will have more peaks etc.\n",
    "\n",
    "\n",
    "# Further Explanation:\n",
    "# we want smoothing because it enables the model to still predict the never-seen combinations \n",
    "# (there should be some chance a bigram, for example, that has 0 counts in the training set, will still be included in the predictions)\n",
    "\n",
    "# Just like andrej discussed about the 'jq' example in the video. \n",
    "# 'jq' combination was never in the training data but we want the model to have some probability of predicting it no matter how low. \n",
    "\n",
    "# This is achieved in a neural network approach by pushing W to be near 0 and not letting it grow too much. \n",
    "# Because, when W is near to zero, logits is near to zero, therefore making logits.exp() near to 1.\n",
    "# Ultimately this makes all the probablities equal making all the combinations have the same chance to appear. \n",
    "\n",
    "# This is why we added regularization. By adding W to loss function, we are making the loss grow with W growing. \n",
    "# Since backward prop tries to decrease the loss, it doesnt let W grow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  783\n",
      "LOSS: 3.761927366256714\n",
      "LOSS: 3.2758822441101074\n",
      "LOSS: 3.0034070014953613\n",
      "LOSS: 2.8422064781188965\n",
      "LOSS: 2.7362682819366455\n",
      "LOSS: 2.660226821899414\n",
      "LOSS: 2.6021833419799805\n",
      "LOSS: 2.555994987487793\n",
      "LOSS: 2.518109083175659\n",
      "LOSS: 2.486332416534424\n",
      "LOSS: 2.4592206478118896\n",
      "LOSS: 2.435776948928833\n",
      "LOSS: 2.4152884483337402\n",
      "LOSS: 2.3972320556640625\n",
      "LOSS: 2.3812108039855957\n",
      "LOSS: 2.366915702819824\n",
      "LOSS: 2.354097843170166\n",
      "LOSS: 2.3425540924072266\n",
      "LOSS: 2.332115411758423\n",
      "LOSS: 2.322638511657715\n",
      "LOSS: 2.3140017986297607\n",
      "LOSS: 2.306100368499756\n",
      "LOSS: 2.2988433837890625\n",
      "LOSS: 2.2921531200408936\n",
      "LOSS: 2.2859628200531006\n",
      "LOSS: 2.280214309692383\n",
      "LOSS: 2.274858236312866\n",
      "LOSS: 2.2698514461517334\n",
      "LOSS: 2.265158176422119\n",
      "LOSS: 2.260746955871582\n",
      "LOSS: 2.256589651107788\n",
      "LOSS: 2.2526631355285645\n",
      "LOSS: 2.248947858810425\n",
      "LOSS: 2.245424509048462\n",
      "LOSS: 2.242079019546509\n",
      "LOSS: 2.238896369934082\n",
      "LOSS: 2.2358651161193848\n",
      "LOSS: 2.23297381401062\n",
      "LOSS: 2.2302134037017822\n",
      "LOSS: 2.2275748252868652\n",
      "LOSS: 2.225050926208496\n",
      "LOSS: 2.2226338386535645\n",
      "LOSS: 2.220318078994751\n",
      "LOSS: 2.2180967330932617\n",
      "LOSS: 2.215965986251831\n",
      "LOSS: 2.2139196395874023\n",
      "LOSS: 2.21195387840271\n",
      "LOSS: 2.210064649581909\n",
      "LOSS: 2.208247661590576\n",
      "LOSS: 2.2065000534057617\n",
      "LOSS: 2.2048180103302\n",
      "LOSS: 2.2031989097595215\n",
      "LOSS: 2.201639413833618\n",
      "LOSS: 2.2001373767852783\n",
      "LOSS: 2.1986892223358154\n",
      "LOSS: 2.197293758392334\n",
      "LOSS: 2.1959478855133057\n",
      "LOSS: 2.194650173187256\n",
      "LOSS: 2.1933979988098145\n",
      "LOSS: 2.1921896934509277\n",
      "LOSS: 2.1910228729248047\n",
      "LOSS: 2.189896583557129\n",
      "LOSS: 2.1888086795806885\n",
      "LOSS: 2.187757968902588\n",
      "LOSS: 2.1867425441741943\n",
      "LOSS: 2.185760974884033\n",
      "LOSS: 2.184812068939209\n",
      "LOSS: 2.183894395828247\n",
      "LOSS: 2.183006525039673\n",
      "LOSS: 2.18214750289917\n",
      "LOSS: 2.1813161373138428\n",
      "LOSS: 2.180511236190796\n",
      "LOSS: 2.1797313690185547\n",
      "LOSS: 2.17897629737854\n",
      "LOSS: 2.1782445907592773\n",
      "LOSS: 2.17753529548645\n",
      "LOSS: 2.1768476963043213\n",
      "LOSS: 2.176180362701416\n",
      "LOSS: 2.1755332946777344\n",
      "LOSS: 2.174905300140381\n",
      "LOSS: 2.174295663833618\n",
      "LOSS: 2.173703670501709\n",
      "LOSS: 2.173128366470337\n",
      "LOSS: 2.1725692749023438\n",
      "LOSS: 2.1720263957977295\n",
      "LOSS: 2.1714980602264404\n",
      "LOSS: 2.1709842681884766\n",
      "LOSS: 2.170484781265259\n",
      "LOSS: 2.1699981689453125\n",
      "LOSS: 2.169524908065796\n",
      "LOSS: 2.1690638065338135\n",
      "LOSS: 2.168614625930786\n",
      "LOSS: 2.1681766510009766\n",
      "LOSS: 2.167750120162964\n",
      "LOSS: 2.1673343181610107\n",
      "LOSS: 2.166928768157959\n",
      "LOSS: 2.1665332317352295\n",
      "LOSS: 2.166146993637085\n",
      "LOSS: 2.1657698154449463\n",
      "LOSS: 2.1654019355773926\n",
      "LOSS: 2.1650424003601074\n",
      "LOSS: 2.164691686630249\n",
      "LOSS: 2.164348840713501\n",
      "LOSS: 2.164013624191284\n",
      "LOSS: 2.1636860370635986\n",
      "LOSS: 2.163365364074707\n",
      "LOSS: 2.1630520820617676\n",
      "LOSS: 2.162745475769043\n",
      "LOSS: 2.162445068359375\n",
      "LOSS: 2.162151575088501\n",
      "LOSS: 2.1618642807006836\n",
      "LOSS: 2.1615829467773438\n",
      "LOSS: 2.1613070964813232\n",
      "LOSS: 2.161036968231201\n",
      "LOSS: 2.1607723236083984\n",
      "LOSS: 2.160512924194336\n",
      "LOSS: 2.1602587699890137\n",
      "LOSS: 2.1600096225738525\n",
      "LOSS: 2.1597654819488525\n",
      "LOSS: 2.1595258712768555\n",
      "LOSS: 2.159290313720703\n",
      "LOSS: 2.159059762954712\n",
      "LOSS: 2.1588337421417236\n",
      "LOSS: 2.158611297607422\n",
      "LOSS: 2.158393144607544\n",
      "LOSS: 2.15817928314209\n",
      "LOSS: 2.1579689979553223\n",
      "LOSS: 2.157762289047241\n",
      "LOSS: 2.157559633255005\n",
      "LOSS: 2.157360553741455\n",
      "LOSS: 2.1571645736694336\n",
      "LOSS: 2.1569721698760986\n",
      "LOSS: 2.15678334236145\n",
      "LOSS: 2.15659761428833\n",
      "LOSS: 2.156414747238159\n",
      "LOSS: 2.1562349796295166\n",
      "LOSS: 2.1560583114624023\n",
      "LOSS: 2.1558845043182373\n",
      "LOSS: 2.1557135581970215\n",
      "LOSS: 2.155545234680176\n",
      "LOSS: 2.1553800106048584\n",
      "LOSS: 2.155217409133911\n",
      "LOSS: 2.155056953430176\n",
      "LOSS: 2.1548988819122314\n",
      "LOSS: 2.1547436714172363\n",
      "LOSS: 2.1545908451080322\n",
      "LOSS: 2.15444016456604\n",
      "LOSS: 2.154292106628418\n",
      "LOSS: 2.1541459560394287\n",
      "LOSS: 2.1540024280548096\n",
      "LOSS: 2.1538608074188232\n",
      "LOSS: 2.1537210941314697\n",
      "LOSS: 2.153583526611328\n",
      "LOSS: 2.1534481048583984\n",
      "LOSS: 2.1533143520355225\n",
      "LOSS: 2.1531827449798584\n",
      "LOSS: 2.1530532836914062\n",
      "LOSS: 2.1529252529144287\n",
      "LOSS: 2.152798891067505\n",
      "LOSS: 2.152674674987793\n",
      "LOSS: 2.1525521278381348\n",
      "LOSS: 2.1524312496185303\n",
      "LOSS: 2.1523115634918213\n",
      "LOSS: 2.1521944999694824\n",
      "LOSS: 2.152078151702881\n",
      "LOSS: 2.151963472366333\n",
      "LOSS: 2.151850700378418\n",
      "LOSS: 2.1517393589019775\n",
      "LOSS: 2.1516292095184326\n",
      "LOSS: 2.1515207290649414\n",
      "LOSS: 2.1514134407043457\n",
      "LOSS: 2.1513075828552246\n",
      "LOSS: 2.151203155517578\n",
      "LOSS: 2.1511003971099854\n",
      "LOSS: 2.15099835395813\n",
      "LOSS: 2.150897979736328\n",
      "LOSS: 2.150799036026001\n",
      "LOSS: 2.150700807571411\n",
      "LOSS: 2.150604248046875\n",
      "LOSS: 2.150508403778076\n",
      "LOSS: 2.150414228439331\n",
      "LOSS: 2.1503207683563232\n",
      "LOSS: 2.15022873878479\n",
      "LOSS: 2.150137424468994\n",
      "LOSS: 2.150047540664673\n",
      "LOSS: 2.149958610534668\n",
      "LOSS: 2.1498711109161377\n",
      "LOSS: 2.1497840881347656\n",
      "LOSS: 2.149698495864868\n",
      "LOSS: 2.149613857269287\n",
      "LOSS: 2.1495299339294434\n",
      "LOSS: 2.149447202682495\n",
      "LOSS: 2.1493654251098633\n",
      "LOSS: 2.1492841243743896\n",
      "LOSS: 2.1492040157318115\n",
      "LOSS: 2.149125099182129\n",
      "LOSS: 2.1490466594696045\n",
      "LOSS: 2.1489691734313965\n",
      "LOSS: 2.148892641067505\n",
      "LOSS: 2.1488170623779297\n"
     ]
    }
   ],
   "source": [
    "############ COMPLETE ITERATIVE IMPLEMENTATION OF THE MODEL ##############\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# create the dataset\n",
    "with open('names.txt', 'r') as file:\n",
    "    names = file.readlines()\n",
    "names = [name.strip().lower() for name in names] # only lowercase letters to get 26 chars\n",
    "\n",
    "SPECIAL_CH = '.'\n",
    "\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)} # {'a':1, 'b':2, 'c':3, ..., 'z':26}\n",
    "stoi[SPECIAL_CH] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "NUM_CHARS = len(chars + [SPECIAL_CH])\n",
    "\n",
    "xs, ys = [], []\n",
    "for name in names:\n",
    "    chs = [SPECIAL_CH] + list(name) + [SPECIAL_CH]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1) \n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs) # first chars of each pair from dataset - one dimensional array [0,5,13,13,1,...]\n",
    "ys = torch.tensor(ys) # chars following the first char from the pairs\n",
    "num_chars_to_sample = xs.nelement()\n",
    "print('number of examples: ',num_chars_to_sample) # how many chars to sample predictions for    \n",
    "\n",
    "# Initialize Network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((NUM_CHARS,NUM_CHARS), generator=g, requires_grad=True) # start with random weights\n",
    "\n",
    "# print(F.one_hot(xs, num_classes=NUM_CHARS).float())\n",
    "# print(xs)\n",
    "\n",
    "############################################ \n",
    "#               GRADIENT DESCENT           # \n",
    "# ##########################################\n",
    "\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=NUM_CHARS).float() # one hot encode the input to the network - each row (one row per char) is 0s with the integer to str mapping idx set as 1\n",
    "    logits = xenc @ W # logits is the appropriate row of W to find the counts/prob for that char/pair. W is the log counts. (the original bigram table with the counts would be W exponentiated - W.exp())\n",
    "    counts = logits.exp() # make all positive - e^x, vals close to zero will be close to 1\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probability for next char\n",
    "    regularization_strength = 0.01 # can adjust this strength. the higher it is the more smooth it makes the distribution (more uniform). If higher it dominates the loss fn below and will make the weights (W) unable to grow because too much loss will be accumulated. everything will become uniform distribution equal predictions (?)\n",
    "    loss = -probs[torch.arange(xs.nelement()), ys].log().mean() + regularization_strength*(W**2).mean() # regularizatization wants to push towards 0\n",
    "    print(f'LOSS: {loss.item()}') # we should see the loss decreasing\n",
    "\n",
    "    ######### BACKWARD PASS ###############\n",
    "    # Zero the gradient\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    ######### UPDATE THE WEIGHTS #############\n",
    "    learning_rate = 50 # if slow loss reduction, increase the learning rate to bring it down faster\n",
    "    \n",
    "    W.data += -learning_rate * W.grad # go in reverse direction of gradient with the goal of reducing loss\n",
    "\n",
    "\n",
    "    # Note: the W matrix of counts (log counts) when exponentiated is the same as the Bigram grid in bigramllm.ipynb\n",
    "    # ***** Where in bigramllm.ipynb the Matrix was created by manually getting counts, here the W matrix with the same counts was arrived at via following the loss and adjusting during gradient descent ******\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2.718282,     7.389056,     0.000045,     0.999000])\n"
     ]
    }
   ],
   "source": [
    "# t = torch.tensor([1,2,-10,-.001])\n",
    "# torch.set_printoptions(precision=6, sci_mode=False)\n",
    "# print(t.exp())\n",
    "\n",
    "# torch.set_printoptions(profile=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m###### SAMPLE FROM THE NEURAL NET MODEL ######\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mGenerator()\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m2147483647\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      6\u001b[0m     out \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "###### SAMPLE FROM THE NEURAL NET MODEL ######\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "\n",
    "    while True:\n",
    "        # plucks out the row of W corresponding to ix - will be the first char, then the char predicted, and the char predicted after that..etc..\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=NUM_CHARS).float()\n",
    "        logits = xenc @ W # one hot encoded char multiplied by the weights in W\n",
    "        counts = logits.exp() # getting the exponentiated log counts for the chars\n",
    "\n",
    "        p = counts / counts.sum(1, keepdims=True) # probabilities for next char\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() # gets a char based on the probabilities calculated above\n",
    "        out.append(itos[ix])\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# missing ca bigram\n",
    "with open('simple_missing_one_combo_in_dataset.txt', 'r') as file:\n",
    "    names = file.readlines()\n",
    "names = [name.strip().lower() for name in names] # only lowercase letters to get 26 chars\n",
    "\n",
    "SPECIAL_CH = '.'\n",
    "\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)} # {'a':1, 'b':2, 'c':3, ..., 'z':26}\n",
    "stoi[SPECIAL_CH] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "NUM_CHARS = len(chars + [SPECIAL_CH])\n",
    "\n",
    "xs, ys = [], []\n",
    "for name in names:\n",
    "    chs = [SPECIAL_CH] + list(name) + [SPECIAL_CH]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1) \n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs) # first chars of each pair from dataset - one dimensional array [0,5,13,13,1,...]\n",
    "ys = torch.tensor(ys) # chars following the first char from the pairs\n",
    "num_chars_to_sample = xs.nelement()\n",
    "print('number of examples: ',num_chars_to_sample) # how many chars to sample predictions for    \n",
    "\n",
    "# Initialize Network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((NUM_CHARS,NUM_CHARS), generator=g, requires_grad=True) # start with random weights\n",
    "\n",
    "# print(F.one_hot(xs, num_classes=NUM_CHARS).float())\n",
    "# print(xs)\n",
    "\n",
    "############################################ \n",
    "#               GRADIENT DESCENT           # \n",
    "# ##########################################\n",
    "\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=NUM_CHARS).float() # one hot encode the input to the network - each row (one row per char) is 0s with the integer to str mapping idx set as 1\n",
    "    logits = xenc @ W # logits is the appropriate row of W to find the counts/prob for that char/pair. W is the log counts. (the original bigram table with the counts would be W exponentiated - W.exp())\n",
    "    counts = logits.exp() # make all positive - e^x, vals close to zero will be close to 1\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probability for next char\n",
    "    regularization_strength = 0.01 # can adjust this strength. the higher it is the more smooth it makes the distribution (more uniform). If higher it dominates the loss fn below and will make the weights (W) unable to grow because too much loss will be accumulated. everything will become uniform distribution equal predictions (?)\n",
    "    loss = -probs[torch.arange(xs.nelement()), ys].log().mean() + regularization_strength*(W**2).mean() # regularizatization wants to push towards 0\n",
    "    print(f'LOSS: {loss.item()}') # we should see the loss decreasing\n",
    "\n",
    "    ######### BACKWARD PASS ###############\n",
    "    # Zero the gradient\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    ######### UPDATE THE WEIGHTS #############\n",
    "    learning_rate = 50 # if slow loss reduction, increase the learning rate to bring it down faster\n",
    "    \n",
    "    W.data += -learning_rate * W.grad # go in reverse direction of gradient with the goal of reducing loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
