{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: ['aa', 'ab', 'bb']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# simple training set names based on 2 chars in dataset: a,b. This training set is missing the `ba` bigram\n",
    "names = ['aa','ab','bb']\n",
    "print(f'training set: {names}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_CHARS=2\n",
      "stoi={'a': 0, 'b': 1, '.': 0}\n",
      "itos={0: '.', 1: 'b'}\n"
     ]
    }
   ],
   "source": [
    "SPECIAL_CH = '.'\n",
    "\n",
    "chars = sorted(list(set(''.join(names)))) # unique chars in the training set\n",
    "stoi = {s:i for i, s in enumerate(chars)} # {'a':1, 'b':2, ...}\n",
    "# use 0 based indexing for use with F.one_hot (otherwise we get error about num_classes being too small with 2 num_classes)\n",
    "\n",
    "stoi[SPECIAL_CH] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "# NUM_CHARS = len(chars + [SPECIAL_CH])\n",
    "NUM_CHARS = len(chars)\n",
    "\n",
    "print(f'{NUM_CHARS=}')\n",
    "print(f'{stoi=}')\n",
    "print(f'{itos=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first letter in each pair found - xs=[0 0 1]\n",
      "second letter in each pair found - ys=[0 1 1]\n",
      "number of char pairs in the training set:  3\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], [] # xs are first chars, ys are second chars found that follow the first char\n",
    "\n",
    "for name in names:\n",
    "    # chs = [SPECIAL_CH] + list(name) + [SPECIAL_CH] # taking each name from training set and surrounding it with special start/end char\n",
    "    chs = list(name) # taking each name from training set and surrounding it with special start/end char\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): # loop through the pairs of chars in each name in the training set, collect pairs of chars that occur in the xs and ys\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1) # list of integer pairs found in the training set - x is first char (input), y is the target char (output,truth that follows first char)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs) # first chars of each pair from dataset - one dimensional array [0,5,13,13,1,...]\n",
    "ys = torch.tensor(ys) # integer representation of chars following the first char from the pairs\n",
    "\n",
    "print(f'first letter in each pair found - xs={xs.numpy()}') # convert to numpy array for easy printing. these are the int representations of first chars in a pair found in the training set\n",
    "print(f'second letter in each pair found - ys={ys.numpy()}') # second char corresponding to first xs char in a pair in the trainingset\n",
    "\n",
    "num_pairs_found = xs.nelement()\n",
    "print('number of char pairs in the training set: ',num_pairs_found) # how many char pairs to sample predictions for    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (randomly initialized):\n",
      "[[-0.98000735 -1.6578479 ]\n",
      " [-0.05716623 -0.3408541 ]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((NUM_CHARS,NUM_CHARS), generator=g, requires_grad=True) # start with random weights - one column/row for each char (incl special)\n",
    "# NUM_CHARS is the number of all unique characters found in the training set plus one special char used for denoting begin or end of a name\n",
    "print('Weights (randomly initialized):')\n",
    "print(W.detach().numpy()) # detach needed because of the require grad option on the tensor before converting to numpy\n",
    "\n",
    "\n",
    "# Each row in W represents the weights (or logits) for predicting the next character given the current character. Specifically:\n",
    "# W[i, :] contains the logits for predicting all possible next characters when the current character is the one at index i in your vocabulary.\n",
    "# If i corresponds to 'b', then probs[i, :] shows the distribution of probabilities for 'a', 'b', 'c' (or whatever characters you have) following 'b'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n",
      "tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# one hot encoded first chars of each member of the pairs that occur in the training set\n",
    "# [0,1,0,0] => 'a'\n",
    "print(xs.detach().numpy()) # int representation of the char\n",
    "print(F.one_hot(xs, num_classes=NUM_CHARS).float()) # which \"bit\" of 2 possible bits (one per unique char) is turned on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot encoded: tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]])\n",
      "W: [[-0.98000735 -1.6578479 ]\n",
      " [-0.05716623 -0.3408541 ]]\n",
      "logits: [[-0.98000735 -1.6578479 ]\n",
      " [-0.98000735 -1.6578479 ]\n",
      " [-0.05716623 -0.3408541 ]]\n",
      "LOSS: 0.7909190654754639\n",
      "one hot encoded: tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]])\n",
      "W: [[-6.176892   4.1984987]\n",
      " [-9.550377   9.251863 ]]\n",
      "logits: [[-6.176892   4.1984987]\n",
      " [-6.176892   4.1984987]\n",
      " [-9.550377   9.251863 ]]\n",
      "LOSS: 4.039954662322998\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############################################ \n",
    "#               GRADIENT DESCENT           # \n",
    "# ##########################################\n",
    "\n",
    "for k in range(2):\n",
    "    ############# FORWARD PASS #############\n",
    "    xenc = F.one_hot(xs, num_classes=NUM_CHARS).float() # each row represents a char (one row per char in each name in training dataset) is 0s with the integer to str mapping idx set as 1\n",
    "    print(f'one hot encoded: {F.one_hot(xs, num_classes=NUM_CHARS).float()}')\n",
    "    print(f'W: {W.detach().numpy()}')\n",
    "\n",
    "    # Note on logits term: the term \"logits\" is sometimes associated with log-odds, but in neural networks, they are simply the raw output scores before applying an activation function\n",
    "    \n",
    "    # this is feeding inputs into one layer of a neuarl network see timestamp 1:18:52 in https://www.youtube.com/watch?v=PaCmpygFfXo&lc=Ugyhw4PpaUFzfqrHcy14AaABAg.AEW5tZ9Y2CWAEd2P4GJO7e\n",
    "    logits = xenc @ W # logits is the appropriate row of W to find the counts/prob for that char/pair. W is the log counts. (the original bigram table with the counts would be W exponentiated - W.exp())\n",
    "    # in matrix multiplication we retain the original number of rows of Matrix A (xenc), so we'll have 24 rows and 4 columns\n",
    "    print(f'logits: {logits.detach().numpy()}')\n",
    "    \n",
    "\n",
    "\n",
    "    ############ START SOFTMAX ###################\n",
    "\n",
    "    # softmax  - see timestamp 1:27:50\n",
    "    # expontentiation turns negative to positive numbers and makes all of the results sum to 1 (normalizes for probability usage)\n",
    "    # Each element in the resulting vector represents a raw score for a potential next character\n",
    "    scores = logits.exp() # make all positive - e^x, vals close to zero will be close to 1\n",
    "    \n",
    "    # The 1 passed as the first argument to sum() specifies that we're summing along dimension 1, which is the row dimension (0 would be columns)\n",
    "    # these probabalities have a row for every example that sums to 1\n",
    "    # takes all of the scores and turns them into probabilities based on the entire score set for chars per row for following a particular x char\n",
    "    probs = scores / scores.sum(1, keepdims=True) # probability for next char\n",
    "    \n",
    "    ############# END SOFTMAX ###################\n",
    "\n",
    "      \n",
    "    # gets the corresponding y char (matching the pair with preceeding x char) in order (first x char -> y pair char)\n",
    "    # arange of number of xs is just a helper to get the indexes from 0 to n xs so we index into probs from the first row to the last. The first row was still derived from the first x value, not the first index of itos\n",
    "    # first row is derived from first 'x' char. ys is the corresponding index y for that x char\n",
    "    # torch.arange(num_xs) here is just a placeholder sequence of indexes to go from the top to the bottom of the probs matrix\n",
    "    # torch.arange(xs.nelement()) simply generates a sequence from 0 to n-1 where n is the number of elements in xs. This sequence is used to index into the rows of probs. It does not match up based on the character's identity ('b', 'a', etc.) but rather on the order in which they appear in xs\n",
    "    xs_rows = xs.nelement()\n",
    "    regularization_strength = 0.01 # can adjust this strength. the higher it is the more smooth it makes the distribution (more uniform). If higher it dominates the loss fn below and will make the weights (W) unable to grow because too much loss will be accumulated. everything will become uniform distribution equal predictions (?)\n",
    "    loss = -probs[torch.arange(xs_rows), ys].log().mean() + regularization_strength*(W**2).mean() # regularizatization wants to push towards 0\n",
    "    \n",
    "    print(f'LOSS: {loss.item()}') # we should see the loss decreasing\n",
    "    \n",
    "    ######### BACKWARD PASS ###############\n",
    "    W.grad = None # Zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    ######### UPDATE THE WEIGHTS #############\n",
    "    learning_rate = 50 # if slow loss reduction, increase the learning rate to bring it down faster\n",
    "    W.data += -learning_rate * W.grad # go in reverse direction of gradient with the goal of reducing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logic behind how the probabilities are calculated and accessed in this neural network implementation for bigram language modeling:\n",
    "\n",
    "# Understanding the Probability Matrix (probs)\n",
    "# One-hot Encoding (xenc):\n",
    "# xenc is created by converting each input character (xs) into a one-hot vector. This means for each character in xs, you get a vector where all elements are zero except for one position which is 1, corresponding to the character's index in stoi.\n",
    "# Logits and Scores:\n",
    "# logits = xenc @ W: Here, each one-hot vector is multiplied by the weight matrix W. This operation gives you raw, unnormalized scores (logits) for each possible next character given the current character. The shape of logits will be [number of examples, NUM_CHARS].\n",
    "# Softmax Transformation:\n",
    "# scores = logits.exp(): Exponentiation to get positive scores.\n",
    "# probs = scores / scores.sum(1, keepdims=True): Softmax is applied to turn these scores into probabilities that sum to 1 for each input example. Now, probs is a matrix where probs[i, j] represents the probability of the j-th character following the i-th input character.\n",
    "\n",
    "# Indexing into probs\n",
    "# Row Index: Each row in probs corresponds to one example from xs. If xs[i] represents the character at index i, then row i in probs gives the probabilities of what character could follow this character.\n",
    "# Column Index: Each column in probs corresponds to a possible next character from the vocabulary (NUM_CHARS). \n",
    "# Accessing Probability for the Actual Next Character:\n",
    "# probs[torch.arange(xs.nelement()), ys] does the following:\n",
    "# torch.arange(xs.nelement()) creates an array [0, 1, 2, ..., n-1] where n is the number of examples. This selects the correct row for each example.\n",
    "# ys contains the indices of the characters that actually followed each xs in the training data. Therefore, ys[i] is the correct column for the i-th example.\n",
    "# This indexing operation thus picks out the probability that the model assigned to the true next character for each input character.\n",
    "\n",
    "# Why This Works:\n",
    "# The structure of probs is designed such that for any given input character (represented by a row), the probabilities across the columns tell you the likelihood of each possible next character. \n",
    "# By selecting probs[i, ys[i]] for each i, you're essentially checking how well the model predicted the actual next character (ys[i]) given the current character (xs[i]).\n",
    "\n",
    "# This approach is fundamental in training language models where the goal is to maximize the probability of the next correct character, thereby reducing the cross-entropy loss. Here, the indexing directly aligns with this goal by focusing on the exact probabilities we're interested in for loss calculation.\n",
    "\n",
    "\n",
    "\n",
    "# Example Walkthrough:\n",
    "# If xs = ['b', 'a', 'c']:\n",
    "# torch.arange(xs.nelement()) would be [0, 1, 2].\n",
    "# probs[0, :] corresponds to the probabilities for characters following 'b' (first x).\n",
    "# probs[1, :] corresponds to the probabilities for characters following 'a' (second x).\n",
    "# probs[2, :] corresponds to the probabilities for characters following 'c' (third x).\n",
    "# ys then tells you which column to look at for each of these rows:\n",
    "# If ys = ['c', 'b', 'a'], then:\n",
    "# probs[0, stoi['c']] gives the probability of 'c' following 'b'.\n",
    "# probs[1, stoi['b']] gives the probability of 'b' following 'a'.\n",
    "# probs[2, stoi['a']] gives the probability of 'a' following 'c'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
