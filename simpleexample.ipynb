{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'ab', 'ac', 'bb', 'ba', 'bc', 'cc', 'cb']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# simple training set names based on 3 chars in dataset: a,b,c. This training set is missing the `ca` bigram\n",
    "names = ['aa', 'ab', 'ac', 'bb', 'ba', 'bc', 'cc', 'cb']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_CHARS=4\n",
      "stoi={'a': 1, 'b': 2, 'c': 3, '.': 0}\n",
      "itos={1: 'a', 2: 'b', 3: 'c', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "SPECIAL_CH = '.'\n",
    "\n",
    "chars = sorted(list(set(''.join(names)))) # unique chars in the training set\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)} # {'a':1, 'b':2, 'c':3, ..., 'z':26}\n",
    "stoi[SPECIAL_CH] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "NUM_CHARS = len(chars + [SPECIAL_CH])\n",
    "\n",
    "print(f'{NUM_CHARS=}')\n",
    "print(f'{stoi=}')\n",
    "print(f'{itos=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs=[0 1 1 0 1 2 0 1 3 0 2 2 0 2 1 0 2 3 0 3 3 0 3 2]\n",
      "ys=[1 1 0 1 2 0 1 3 0 2 2 0 2 1 0 2 3 0 3 3 0 3 2 0]\n",
      "number of char pairs in the training set:  24\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], [] # xs are first chars, ys are second chars found that follow the first char\n",
    "\n",
    "for name in names:\n",
    "    chs = [SPECIAL_CH] + list(name) + [SPECIAL_CH] # taking each name from training set and surrounding it with special start/end char\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): # loop through the pairs of chars in each name in the training set, collect pairs of chars that occur in the xs and ys\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1) # list of integer pairs found in the training set - x is first char (input), y is the target char (output,truth that follows first char)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs) # first chars of each pair from dataset - one dimensional array [0,5,13,13,1,...]\n",
    "ys = torch.tensor(ys) # chars following the first char from the pairs\n",
    "\n",
    "print(f'xs={xs.numpy()}') # convert to numpy array for easy printing. these are the int representations of first chars in a pair found in the training set\n",
    "print(f'ys={ys.numpy()}') # second char corresponding to first xs char in a pair in the trainingset\n",
    "\n",
    "num_pairs_found = xs.nelement()\n",
    "print('number of char pairs in the training set: ',num_pairs_found) # how many char pairs to sample predictions for    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (randomly initialized):\n",
      "[[ 1.567362   -0.23729232 -0.02738461 -1.1007794 ]\n",
      " [ 0.28588146 -0.02964334 -1.5470592   0.60489196]\n",
      " [ 0.0791362   0.90462387 -0.4712532   0.786822  ]\n",
      " [-0.32843494 -0.43297017  1.3729309   2.9333673 ]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((NUM_CHARS,NUM_CHARS), generator=g, requires_grad=True) # start with random weights - one column/row for each char (incl special)\n",
    "# NUM_CHARS is the number of all unique characters found in the training set plus one special char used for denoting begin or end of a name\n",
    "print('Weights (randomly initialized):')\n",
    "print(W.detach().numpy()) # detach needed because of the require grad option on the tensor before converting to numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1 2 0 1 3 0 2 2 0 2 1 0 2 3 0 3 3 0 3 2]\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# one hot encoded first chars of each member of the pairs that occur in the training set\n",
    "# [0,1,0,0] => 'a'\n",
    "print(xs.detach().numpy()) # int representation of the char\n",
    "print(F.one_hot(xs, num_classes=NUM_CHARS).float()) # which \"bit\" of 4 possible bits (one per unique char) is turned on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot encoded: tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.]])\n",
      "W: [[-5.667372    5.0118155  -1.1700845   1.9315311 ]\n",
      " [-1.7576466   0.4487791   0.49110973  0.45801425]\n",
      " [-1.7325449  -1.4437535   2.287304    1.5704437 ]\n",
      " [ 1.6058083  -1.0105455   2.8309557  -1.5670612 ]]\n",
      "logits: [[-5.667372    5.0118155  -1.1700845   1.9315311 ]\n",
      " [-1.7576466   0.4487791   0.49110973  0.45801425]\n",
      " [-1.7576466   0.4487791   0.49110973  0.45801425]\n",
      " [-5.667372    5.0118155  -1.1700845   1.9315311 ]\n",
      " [-1.7576466   0.4487791   0.49110973  0.45801425]\n",
      " [-1.7325449  -1.4437535   2.287304    1.5704437 ]\n",
      " [-5.667372    5.0118155  -1.1700845   1.9315311 ]\n",
      " [-1.7576466   0.4487791   0.49110973  0.45801425]\n",
      " [ 1.6058083  -1.0105455   2.8309557  -1.5670612 ]\n",
      " [-5.667372    5.0118155  -1.1700845   1.9315311 ]\n",
      " [-1.7325449  -1.4437535   2.287304    1.5704437 ]\n",
      " [-1.7325449  -1.4437535   2.287304    1.5704437 ]\n",
      " [-5.667372    5.0118155  -1.1700845   1.9315311 ]\n",
      " [-1.7325449  -1.4437535   2.287304    1.5704437 ]\n",
      " [-1.7576466   0.4487791   0.49110973  0.45801425]\n",
      " [-5.667372    5.0118155  -1.1700845   1.9315311 ]\n",
      " [-1.7325449  -1.4437535   2.287304    1.5704437 ]\n",
      " [ 1.6058083  -1.0105455   2.8309557  -1.5670612 ]\n",
      " [-5.667372    5.0118155  -1.1700845   1.9315311 ]\n",
      " [ 1.6058083  -1.0105455   2.8309557  -1.5670612 ]\n",
      " [ 1.6058083  -1.0105455   2.8309557  -1.5670612 ]\n",
      " [-5.667372    5.0118155  -1.1700845   1.9315311 ]\n",
      " [ 1.6058083  -1.0105455   2.8309557  -1.5670612 ]\n",
      " [-1.7325449  -1.4437535   2.287304    1.5704437 ]]\n",
      "LOSS: 2.714317560195923\n",
      "one hot encoded: tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.]])\n",
      "W: [[-5.3135276  -4.954188    5.120182    5.2468057 ]\n",
      " [ 2.1562588  -0.7896285  -0.89236057 -0.81152916]\n",
      " [ 4.4790626   0.53402793 -3.9411535  -0.43307817]\n",
      " [ 5.45068    -1.1157929  -3.109609    0.51768255]]\n",
      "logits: [[-5.3135276  -4.954188    5.120182    5.2468057 ]\n",
      " [ 2.1562588  -0.7896285  -0.89236057 -0.81152916]\n",
      " [ 2.1562588  -0.7896285  -0.89236057 -0.81152916]\n",
      " [-5.3135276  -4.954188    5.120182    5.2468057 ]\n",
      " [ 2.1562588  -0.7896285  -0.89236057 -0.81152916]\n",
      " [ 4.4790626   0.53402793 -3.9411535  -0.43307817]\n",
      " [-5.3135276  -4.954188    5.120182    5.2468057 ]\n",
      " [ 2.1562588  -0.7896285  -0.89236057 -0.81152916]\n",
      " [ 5.45068    -1.1157929  -3.109609    0.51768255]\n",
      " [-5.3135276  -4.954188    5.120182    5.2468057 ]\n",
      " [ 4.4790626   0.53402793 -3.9411535  -0.43307817]\n",
      " [ 4.4790626   0.53402793 -3.9411535  -0.43307817]\n",
      " [-5.3135276  -4.954188    5.120182    5.2468057 ]\n",
      " [ 4.4790626   0.53402793 -3.9411535  -0.43307817]\n",
      " [ 2.1562588  -0.7896285  -0.89236057 -0.81152916]\n",
      " [-5.3135276  -4.954188    5.120182    5.2468057 ]\n",
      " [ 4.4790626   0.53402793 -3.9411535  -0.43307817]\n",
      " [ 5.45068    -1.1157929  -3.109609    0.51768255]\n",
      " [-5.3135276  -4.954188    5.120182    5.2468057 ]\n",
      " [ 5.45068    -1.1157929  -3.109609    0.51768255]\n",
      " [ 5.45068    -1.1157929  -3.109609    0.51768255]\n",
      " [-5.3135276  -4.954188    5.120182    5.2468057 ]\n",
      " [ 5.45068    -1.1157929  -3.109609    0.51768255]\n",
      " [ 4.4790626   0.53402793 -3.9411535  -0.43307817]]\n",
      "LOSS: 3.3138368129730225\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############################################ \n",
    "#               GRADIENT DESCENT           # \n",
    "# ##########################################\n",
    "\n",
    "for k in range(2):\n",
    "    ############# FORWARD PASS #############\n",
    "    xenc = F.one_hot(xs, num_classes=NUM_CHARS).float() # each row represents a char (one row per char in each name in training dataset) is 0s with the integer to str mapping idx set as 1\n",
    "    print(f'one hot encoded: {F.one_hot(xs, num_classes=NUM_CHARS).float()}')\n",
    "    print(f'W: {W.detach().numpy()}')\n",
    "    logits = xenc @ W # logits is the appropriate row of W to find the counts/prob for that char/pair. W is the log counts. (the original bigram table with the counts would be W exponentiated - W.exp())\n",
    "    # in matrix multiplication we retain the original number of rows of Matrix A (xenc), so we'll have 24 rows and 4 columns\n",
    "    print(f'logits: {logits.detach().numpy()}')\n",
    "    \n",
    "    counts = logits.exp() # make all positive - e^x, vals close to zero will be close to 1\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probability for next char\n",
    "    regularization_strength = 0.01 # can adjust this strength. the higher it is the more smooth it makes the distribution (more uniform). If higher it dominates the loss fn below and will make the weights (W) unable to grow because too much loss will be accumulated. everything will become uniform distribution equal predictions (?)\n",
    "    loss = -probs[torch.arange(xs.nelement()), ys].log().mean() + regularization_strength*(W**2).mean() # regularizatization wants to push towards 0\n",
    "    print(f'LOSS: {loss.item()}') # we should see the loss decreasing\n",
    "    \n",
    "    ######### BACKWARD PASS ###############\n",
    "    W.grad = None # Zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    ######### UPDATE THE WEIGHTS #############\n",
    "    learning_rate = 50 # if slow loss reduction, increase the learning rate to bring it down faster\n",
    "    W.data += -learning_rate * W.grad # go in reverse direction of gradient with the goal of reducing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
