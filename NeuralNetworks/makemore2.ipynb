{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "#   MAKEMORE 2 - https://www.youtube.com/watch?v=TCH_1BHY58I&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=3      #\n",
    "###################################################################################################################\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'liam', 'olivia', 'noah', 'ava', 'ethan', 'sophia', 'mason']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "words = [word.lower() for word in words]\n",
    "# show first few words\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_CHARS=27\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Map words to integers\n",
    "\n",
    "# get unique set of chars in the names dataset\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "NUM_CHARS = len(chars)+1 # number of chars in training set plus special char '.'\n",
    "print(f'{NUM_CHARS=}')\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "liam\n",
      "... ---> l\n",
      "..l ---> i\n",
      ".li ---> a\n",
      "lia ---> m\n",
      "iam ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "noah\n",
      "... ---> n\n",
      "..n ---> o\n",
      ".no ---> a\n",
      "noa ---> h\n",
      "oah ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "ethan\n",
      "... ---> e\n",
      "..e ---> t\n",
      ".et ---> h\n",
      "eth ---> a\n",
      "tha ---> n\n",
      "han ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n",
      "mason\n",
      "... ---> m\n",
      "..m ---> a\n",
      ".ma ---> s\n",
      "mas ---> o\n",
      "aso ---> n\n",
      "son ---> .\n",
      "matthew\n",
      "... ---> m\n",
      "..m ---> a\n",
      ".ma ---> t\n",
      "mat ---> t\n",
      "att ---> h\n",
      "tth ---> e\n",
      "the ---> w\n",
      "hew ---> .\n",
      "modine\n",
      "... ---> m\n",
      "..m ---> o\n",
      ".mo ---> d\n",
      "mod ---> i\n",
      "odi ---> n\n",
      "din ---> e\n",
      "ine ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "william\n",
      "... ---> w\n",
      "..w ---> i\n",
      ".wi ---> l\n",
      "wil ---> l\n",
      "ill ---> i\n",
      "lli ---> a\n",
      "lia ---> m\n",
      "iam ---> .\n",
      "mia\n",
      "... ---> m\n",
      "..m ---> i\n",
      ".mi ---> a\n",
      "mia ---> .\n",
      "james\n",
      "... ---> j\n",
      "..j ---> a\n",
      ".ja ---> m\n",
      "jam ---> e\n",
      "ame ---> s\n",
      "mes ---> .\n",
      "charlotte\n",
      "... ---> c\n",
      "..c ---> h\n",
      ".ch ---> a\n",
      "cha ---> r\n",
      "har ---> l\n",
      "arl ---> o\n",
      "rlo ---> t\n",
      "lot ---> t\n",
      "ott ---> e\n",
      "tte ---> .\n",
      "benjamin\n",
      "... ---> b\n",
      "..b ---> e\n",
      ".be ---> n\n",
      "ben ---> j\n",
      "enj ---> a\n",
      "nja ---> m\n",
      "jam ---> i\n",
      "ami ---> n\n",
      "min ---> .\n",
      "amelia\n",
      "... ---> a\n",
      "..a ---> m\n",
      ".am ---> e\n",
      "ame ---> l\n",
      "mel ---> i\n",
      "eli ---> a\n",
      "lia ---> .\n",
      "lucas\n",
      "... ---> l\n",
      "..l ---> u\n",
      ".lu ---> c\n",
      "luc ---> a\n",
      "uca ---> s\n",
      "cas ---> .\n",
      "harper\n",
      "... ---> h\n",
      "..h ---> a\n",
      ".ha ---> r\n",
      "har ---> p\n",
      "arp ---> e\n",
      "rpe ---> r\n",
      "per ---> .\n",
      "henry\n",
      "... ---> h\n",
      "..h ---> e\n",
      ".he ---> n\n",
      "hen ---> r\n",
      "enr ---> y\n",
      "nry ---> .\n",
      "evelyn\n",
      "... ---> e\n",
      "..e ---> v\n",
      ".ev ---> e\n",
      "eve ---> l\n",
      "vel ---> y\n",
      "ely ---> n\n",
      "lyn ---> .\n",
      "alexander\n",
      "... ---> a\n",
      "..a ---> l\n",
      ".al ---> e\n",
      "ale ---> x\n",
      "lex ---> a\n",
      "exa ---> n\n",
      "xan ---> d\n",
      "and ---> e\n",
      "nde ---> r\n",
      "der ---> .\n",
      "abigail\n",
      "... ---> a\n",
      "..a ---> b\n",
      ".ab ---> i\n",
      "abi ---> g\n",
      "big ---> a\n",
      "iga ---> i\n",
      "gai ---> l\n",
      "ail ---> .\n",
      "michael\n",
      "... ---> m\n",
      "..m ---> i\n",
      ".mi ---> c\n",
      "mic ---> h\n",
      "ich ---> a\n",
      "cha ---> e\n",
      "hae ---> l\n",
      "ael ---> .\n",
      "emily\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> i\n",
      "emi ---> l\n",
      "mil ---> y\n",
      "ily ---> .\n",
      "daniel\n",
      "... ---> d\n",
      "..d ---> a\n",
      ".da ---> n\n",
      "dan ---> i\n",
      "ani ---> e\n",
      "nie ---> l\n",
      "iel ---> .\n",
      "elizabeth\n",
      "... ---> e\n",
      "..e ---> l\n",
      ".el ---> i\n",
      "eli ---> z\n",
      "liz ---> a\n",
      "iza ---> b\n",
      "zab ---> e\n",
      "abe ---> t\n",
      "bet ---> h\n",
      "eth ---> .\n",
      "jacob\n",
      "... ---> j\n",
      "..j ---> a\n",
      ".ja ---> c\n",
      "jac ---> o\n",
      "aco ---> b\n",
      "cob ---> .\n",
      "mila\n",
      "... ---> m\n",
      "..m ---> i\n",
      ".mi ---> l\n",
      "mil ---> a\n",
      "ila ---> .\n",
      "logan\n",
      "... ---> l\n",
      "..l ---> o\n",
      ".lo ---> g\n",
      "log ---> a\n",
      "oga ---> n\n",
      "gan ---> .\n",
      "ella\n",
      "... ---> e\n",
      "..e ---> l\n",
      ".el ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "jackson\n",
      "... ---> j\n",
      "..j ---> a\n",
      ".ja ---> c\n",
      "jac ---> k\n",
      "ack ---> s\n",
      "cks ---> o\n",
      "kso ---> n\n",
      "son ---> .\n",
      "avery\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> e\n",
      "ave ---> r\n",
      "ver ---> y\n",
      "ery ---> .\n",
      "sebastian\n",
      "... ---> s\n",
      "..s ---> e\n",
      ".se ---> b\n",
      "seb ---> a\n",
      "eba ---> s\n",
      "bas ---> t\n",
      "ast ---> i\n",
      "sti ---> a\n",
      "tia ---> n\n",
      "ian ---> .\n",
      "sofia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> f\n",
      "sof ---> i\n",
      "ofi ---> a\n",
      "fia ---> .\n",
      "jack\n",
      "... ---> j\n",
      "..j ---> a\n",
      ".ja ---> c\n",
      "jac ---> k\n",
      "ack ---> .\n",
      "camila\n",
      "... ---> c\n",
      "..c ---> a\n",
      ".ca ---> m\n",
      "cam ---> i\n",
      "ami ---> l\n",
      "mil ---> a\n",
      "ila ---> .\n",
      "aiden\n",
      "... ---> a\n",
      "..a ---> i\n",
      ".ai ---> d\n",
      "aid ---> e\n",
      "ide ---> n\n",
      "den ---> .\n",
      "aria\n",
      "... ---> a\n",
      "..a ---> r\n",
      ".ar ---> i\n",
      "ari ---> a\n",
      "ria ---> .\n",
      "owen\n",
      "... ---> o\n",
      "..o ---> w\n",
      ".ow ---> e\n",
      "owe ---> n\n",
      "wen ---> .\n",
      "scarlett\n",
      "... ---> s\n",
      "..s ---> c\n",
      ".sc ---> a\n",
      "sca ---> r\n",
      "car ---> l\n",
      "arl ---> e\n",
      "rle ---> t\n",
      "let ---> t\n",
      "ett ---> .\n",
      "samuel\n",
      "... ---> s\n",
      "..s ---> a\n",
      ".sa ---> m\n",
      "sam ---> u\n",
      "amu ---> e\n",
      "mue ---> l\n",
      "uel ---> .\n",
      "victoria\n",
      "... ---> v\n",
      "..v ---> i\n",
      ".vi ---> c\n",
      "vic ---> t\n",
      "ict ---> o\n",
      "cto ---> r\n",
      "tor ---> i\n",
      "ori ---> a\n",
      "ria ---> .\n",
      "madison\n",
      "... ---> m\n",
      "..m ---> a\n",
      ".ma ---> d\n",
      "mad ---> i\n",
      "adi ---> s\n",
      "dis ---> o\n",
      "iso ---> n\n",
      "son ---> .\n",
      "joseph\n",
      "... ---> j\n",
      "..j ---> o\n",
      ".jo ---> s\n",
      "jos ---> e\n",
      "ose ---> p\n",
      "sep ---> h\n",
      "eph ---> .\n",
      "luna\n",
      "... ---> l\n",
      "..l ---> u\n",
      ".lu ---> n\n",
      "lun ---> a\n",
      "una ---> .\n",
      "levi\n",
      "... ---> l\n",
      "..l ---> e\n",
      ".le ---> v\n",
      "lev ---> i\n",
      "evi ---> .\n",
      "grace\n",
      "... ---> g\n",
      "..g ---> r\n",
      ".gr ---> a\n",
      "gra ---> c\n",
      "rac ---> e\n",
      "ace ---> .\n",
      "mateo\n",
      "... ---> m\n",
      "..m ---> a\n",
      ".ma ---> t\n",
      "mat ---> e\n",
      "ate ---> o\n",
      "teo ---> .\n",
      "chloe\n",
      "... ---> c\n",
      "..c ---> h\n",
      ".ch ---> l\n",
      "chl ---> o\n",
      "hlo ---> e\n",
      "loe ---> .\n",
      "david\n",
      "... ---> d\n",
      "..d ---> a\n",
      ".da ---> v\n",
      "dav ---> i\n",
      "avi ---> d\n",
      "vid ---> .\n",
      "penelope\n",
      "... ---> p\n",
      "..p ---> e\n",
      ".pe ---> n\n",
      "pen ---> e\n",
      "ene ---> l\n",
      "nel ---> o\n",
      "elo ---> p\n",
      "lop ---> e\n",
      "ope ---> .\n",
      "john\n",
      "... ---> j\n",
      "..j ---> o\n",
      ".jo ---> h\n",
      "joh ---> n\n",
      "ohn ---> .\n",
      "layla\n",
      "... ---> l\n",
      "..l ---> a\n",
      ".la ---> y\n",
      "lay ---> l\n",
      "ayl ---> a\n",
      "yla ---> .\n",
      "wyatt\n",
      "... ---> w\n",
      "..w ---> y\n",
      ".wy ---> a\n",
      "wya ---> t\n",
      "yat ---> t\n",
      "att ---> .\n",
      "riley\n",
      "... ---> r\n",
      "..r ---> i\n",
      ".ri ---> l\n",
      "ril ---> e\n",
      "ile ---> y\n",
      "ley ---> .\n",
      "oliver\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> e\n",
      "ive ---> r\n",
      "ver ---> .\n",
      "zoey\n",
      "... ---> z\n",
      "..z ---> o\n",
      ".zo ---> e\n",
      "zoe ---> y\n",
      "oey ---> .\n",
      "jayden\n",
      "... ---> j\n",
      "..j ---> a\n",
      ".ja ---> y\n",
      "jay ---> d\n",
      "ayd ---> e\n",
      "yde ---> n\n",
      "den ---> .\n",
      "nora\n",
      "... ---> n\n",
      "..n ---> o\n",
      ".no ---> r\n",
      "nor ---> a\n",
      "ora ---> .\n",
      "dylan\n",
      "... ---> d\n",
      "..d ---> y\n",
      ".dy ---> l\n",
      "dyl ---> a\n",
      "yla ---> n\n",
      "lan ---> .\n",
      "lily\n",
      "... ---> l\n",
      "..l ---> i\n",
      ".li ---> l\n",
      "lil ---> y\n",
      "ily ---> .\n",
      "luke\n",
      "... ---> l\n",
      "..l ---> u\n",
      ".lu ---> k\n",
      "luk ---> e\n",
      "uke ---> .\n",
      "eleanor\n",
      "... ---> e\n",
      "..e ---> l\n",
      ".el ---> e\n",
      "ele ---> a\n",
      "lea ---> n\n",
      "ean ---> o\n",
      "ano ---> r\n",
      "nor ---> .\n",
      "gabriel\n",
      "... ---> g\n",
      "..g ---> a\n",
      ".ga ---> b\n",
      "gab ---> r\n",
      "abr ---> i\n",
      "bri ---> e\n",
      "rie ---> l\n",
      "iel ---> .\n",
      "hannah\n",
      "... ---> h\n",
      "..h ---> a\n",
      ".ha ---> n\n",
      "han ---> n\n",
      "ann ---> a\n",
      "nna ---> h\n",
      "nah ---> .\n",
      "anthony\n",
      "... ---> a\n",
      "..a ---> n\n",
      ".an ---> t\n",
      "ant ---> h\n",
      "nth ---> o\n",
      "tho ---> n\n",
      "hon ---> y\n",
      "ony ---> .\n",
      "lillian\n",
      "... ---> l\n",
      "..l ---> i\n",
      ".li ---> l\n",
      "lil ---> l\n",
      "ill ---> i\n",
      "lli ---> a\n",
      "lia ---> n\n",
      "ian ---> .\n",
      "isaac\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> a\n",
      "saa ---> c\n",
      "aac ---> .\n",
      "addison\n",
      "... ---> a\n",
      "..a ---> d\n",
      ".ad ---> d\n",
      "add ---> i\n",
      "ddi ---> s\n",
      "dis ---> o\n",
      "iso ---> n\n",
      "son ---> .\n",
      "grayson\n",
      "... ---> g\n",
      "..g ---> r\n",
      ".gr ---> a\n",
      "gra ---> y\n",
      "ray ---> s\n",
      "ays ---> o\n",
      "yso ---> n\n",
      "son ---> .\n",
      "aubrey\n",
      "... ---> a\n",
      "..a ---> u\n",
      ".au ---> b\n",
      "aub ---> r\n",
      "ubr ---> e\n",
      "bre ---> y\n",
      "rey ---> .\n",
      "julian\n",
      "... ---> j\n",
      "..j ---> u\n",
      ".ju ---> l\n",
      "jul ---> i\n",
      "uli ---> a\n",
      "lia ---> n\n",
      "ian ---> .\n",
      "ellie\n",
      "... ---> e\n",
      "..e ---> l\n",
      ".el ---> l\n",
      "ell ---> i\n",
      "lli ---> e\n",
      "lie ---> .\n",
      "christopher\n",
      "... ---> c\n",
      "..c ---> h\n",
      ".ch ---> r\n",
      "chr ---> i\n",
      "hri ---> s\n",
      "ris ---> t\n",
      "ist ---> o\n",
      "sto ---> p\n",
      "top ---> h\n",
      "oph ---> e\n",
      "phe ---> r\n",
      "her ---> .\n",
      "stella\n",
      "... ---> s\n",
      "..s ---> t\n",
      ".st ---> e\n",
      "ste ---> l\n",
      "tel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "joshua\n",
      "... ---> j\n",
      "..j ---> o\n",
      ".jo ---> s\n",
      "jos ---> h\n",
      "osh ---> u\n",
      "shu ---> a\n",
      "hua ---> .\n",
      "natalie\n",
      "... ---> n\n",
      "..n ---> a\n",
      ".na ---> t\n",
      "nat ---> a\n",
      "ata ---> l\n",
      "tal ---> i\n",
      "ali ---> e\n",
      "lie ---> .\n",
      "andrew\n",
      "... ---> a\n",
      "..a ---> n\n",
      ".an ---> d\n",
      "and ---> r\n",
      "ndr ---> e\n",
      "dre ---> w\n",
      "rew ---> .\n",
      "zoe\n",
      "... ---> z\n",
      "..z ---> o\n",
      ".zo ---> e\n",
      "zoe ---> .\n",
      "lincoln\n",
      "... ---> l\n",
      "..l ---> i\n",
      ".li ---> n\n",
      "lin ---> c\n",
      "inc ---> o\n",
      "nco ---> l\n",
      "col ---> n\n",
      "oln ---> .\n",
      "leah\n",
      "... ---> l\n",
      "..l ---> e\n",
      ".le ---> a\n",
      "lea ---> h\n",
      "eah ---> .\n",
      "ryan\n",
      "... ---> r\n",
      "..r ---> y\n",
      ".ry ---> a\n",
      "rya ---> n\n",
      "yan ---> .\n",
      "hazel\n",
      "... ---> h\n",
      "..h ---> a\n",
      ".ha ---> z\n",
      "haz ---> e\n",
      "aze ---> l\n",
      "zel ---> .\n",
      "nathan\n",
      "... ---> n\n",
      "..n ---> a\n",
      ".na ---> t\n",
      "nat ---> h\n",
      "ath ---> a\n",
      "tha ---> n\n",
      "han ---> .\n",
      "violet\n",
      "... ---> v\n",
      "..v ---> i\n",
      ".vi ---> o\n",
      "vio ---> l\n",
      "iol ---> e\n",
      "ole ---> t\n",
      "let ---> .\n",
      "adam\n",
      "... ---> a\n",
      "..a ---> d\n",
      ".ad ---> a\n",
      "ada ---> m\n",
      "dam ---> .\n",
      "aurora\n",
      "... ---> a\n",
      "..a ---> u\n",
      ".au ---> r\n",
      "aur ---> o\n",
      "uro ---> r\n",
      "ror ---> a\n",
      "ora ---> .\n",
      "leo\n",
      "... ---> l\n",
      "..l ---> e\n",
      ".le ---> o\n",
      "leo ---> .\n",
      "savannah\n",
      "... ---> s\n",
      "..s ---> a\n",
      ".sa ---> v\n",
      "sav ---> a\n",
      "ava ---> n\n",
      "van ---> n\n",
      "ann ---> a\n",
      "nna ---> h\n",
      "nah ---> .\n",
      "jaxon\n",
      "... ---> j\n",
      "..j ---> a\n",
      ".ja ---> x\n",
      "jax ---> o\n",
      "axo ---> n\n",
      "xon ---> .\n",
      "audrey\n",
      "... ---> a\n",
      "..a ---> u\n",
      ".au ---> d\n",
      "aud ---> r\n",
      "udr ---> e\n",
      "dre ---> y\n",
      "rey ---> .\n",
      "isaiah\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> i\n",
      "sai ---> a\n",
      "aia ---> h\n",
      "iah ---> .\n",
      "brooklyn\n",
      "... ---> b\n",
      "..b ---> r\n",
      ".br ---> o\n",
      "bro ---> o\n",
      "roo ---> k\n",
      "ook ---> l\n",
      "okl ---> y\n",
      "kly ---> n\n",
      "lyn ---> .\n",
      "eli\n",
      "... ---> e\n",
      "..e ---> l\n",
      ".el ---> i\n",
      "eli ---> .\n",
      "bella\n",
      "... ---> b\n",
      "..b ---> e\n",
      ".be ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "aaron\n",
      "... ---> a\n",
      "..a ---> a\n",
      ".aa ---> r\n",
      "aar ---> o\n",
      "aro ---> n\n",
      "ron ---> .\n",
      "claire\n",
      "... ---> c\n",
      "..c ---> l\n",
      ".cl ---> a\n",
      "cla ---> i\n",
      "lai ---> r\n",
      "air ---> e\n",
      "ire ---> .\n",
      "carson\n",
      "... ---> c\n",
      "..c ---> a\n",
      ".ca ---> r\n",
      "car ---> s\n",
      "ars ---> o\n",
      "rso ---> n\n",
      "son ---> .\n",
      "skylar\n",
      "... ---> s\n",
      "..s ---> k\n",
      ".sk ---> y\n",
      "sky ---> l\n",
      "kyl ---> a\n",
      "yla ---> r\n",
      "lar ---> .\n",
      "charles\n",
      "... ---> c\n",
      "..c ---> h\n",
      ".ch ---> a\n",
      "cha ---> r\n",
      "har ---> l\n",
      "arl ---> e\n",
      "rle ---> s\n",
      "les ---> .\n",
      "lucy\n",
      "... ---> l\n",
      "..l ---> u\n",
      ".lu ---> c\n",
      "luc ---> y\n",
      "ucy ---> .\n",
      "thomas\n",
      "... ---> t\n",
      "..t ---> h\n",
      ".th ---> o\n",
      "tho ---> m\n",
      "hom ---> a\n",
      "oma ---> s\n",
      "mas ---> .\n",
      "paisley\n",
      "... ---> p\n",
      "..p ---> a\n",
      ".pa ---> i\n",
      "pai ---> s\n",
      "ais ---> l\n",
      "isl ---> e\n",
      "sle ---> y\n",
      "ley ---> .\n",
      "caleb\n",
      "... ---> c\n",
      "..c ---> a\n",
      ".ca ---> l\n",
      "cal ---> e\n",
      "ale ---> b\n",
      "leb ---> .\n",
      "everly\n",
      "... ---> e\n",
      "..e ---> v\n",
      ".ev ---> e\n",
      "eve ---> r\n",
      "ver ---> l\n",
      "erl ---> y\n",
      "rly ---> .\n",
      "josiah\n",
      "... ---> j\n",
      "..j ---> o\n",
      ".jo ---> s\n",
      "jos ---> i\n",
      "osi ---> a\n",
      "sia ---> h\n",
      "iah ---> .\n",
      "anna\n",
      "... ---> a\n",
      "..a ---> n\n",
      ".an ---> n\n",
      "ann ---> a\n",
      "nna ---> .\n",
      "christian\n",
      "... ---> c\n",
      "..c ---> h\n",
      ".ch ---> r\n",
      "chr ---> i\n",
      "hri ---> s\n",
      "ris ---> t\n",
      "ist ---> i\n",
      "sti ---> a\n",
      "tia ---> n\n",
      "ian ---> .\n",
      "caroline\n",
      "... ---> c\n",
      "..c ---> a\n",
      ".ca ---> r\n",
      "car ---> o\n",
      "aro ---> l\n",
      "rol ---> i\n",
      "oli ---> n\n",
      "lin ---> e\n",
      "ine ---> .\n",
      "hunter\n",
      "... ---> h\n",
      "..h ---> u\n",
      ".hu ---> n\n",
      "hun ---> t\n",
      "unt ---> e\n",
      "nte ---> r\n",
      "ter ---> .\n",
      "nova\n",
      "... ---> n\n",
      "..n ---> o\n",
      ".no ---> v\n",
      "nov ---> a\n",
      "ova ---> .\n",
      "landon\n",
      "... ---> l\n",
      "..l ---> a\n",
      ".la ---> n\n",
      "lan ---> d\n",
      "and ---> o\n",
      "ndo ---> n\n",
      "don ---> .\n",
      "genesis\n",
      "... ---> g\n",
      "..g ---> e\n",
      ".ge ---> n\n",
      "gen ---> e\n",
      "ene ---> s\n",
      "nes ---> i\n",
      "esi ---> s\n",
      "sis ---> .\n",
      "jonathan\n",
      "... ---> j\n",
      "..j ---> o\n",
      ".jo ---> n\n",
      "jon ---> a\n",
      "ona ---> t\n",
      "nat ---> h\n",
      "ath ---> a\n",
      "tha ---> n\n",
      "han ---> .\n",
      "emilia\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> i\n",
      "emi ---> l\n",
      "mil ---> i\n",
      "ili ---> a\n",
      "lia ---> .\n",
      "quinn\n",
      "... ---> q\n",
      "..q ---> u\n",
      ".qu ---> i\n",
      "qui ---> n\n",
      "uin ---> n\n",
      "inn ---> .\n",
      "X shape:  torch.Size([790, 3])\n",
      "Y shape:  torch.Size([790])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "# block size is the context length - how many characters we use to predict the next one\n",
    "block_size = 3 # get the Y character given the previous {block_size} number of characters...\n",
    "# X are inputs and Y are the labels for each example X\n",
    "X,Y = [],[]\n",
    "# print examples of x input with their possible labels\n",
    "for w in words:\n",
    "    print(w)\n",
    "    context = [0] * block_size # [0,0,0] \n",
    "    for ch in w + '.': # loop chars in word - add . to the word 'emma.'\n",
    "        ix = stoi[ch] # number repr. of the character\n",
    "        X.append(context) # add the block of characters that map to the Y label next character # [0,0,0] -> [E_numrepr]\n",
    "        Y.append(ix) # add number repr of character to the label list\n",
    "        print(''.join(itos[num] for num in context), '--->', itos[ix]) # the context and the next prediction label\n",
    "        context = context[1:] + [ix] # add the label char (what comes next in the training set) to the current context and move to the next step over in the context to get next window of 3 chars to get next three chars and prediction label\n",
    "\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "print('X shape: ', X.shape)\n",
    "print('Y shape: ', Y.shape)\n",
    "# print(Y)\n",
    "\n",
    "assert X.shape[0] == Y.shape[0]\n",
    "USE_THIS = X.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([790, 3]) - Number of examples, size of an input into the neural net (block size number of ints)\n",
      "type of data of X/Y: torch.int64\n",
      "torch.Size([790]) - number of labels\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 5]])\n",
      "tensor([ 5, 13])\n"
     ]
    }
   ],
   "source": [
    "# Number of examples - each input to the neural net is 3 integers (or whatever the block size is), the type of data\n",
    "print(f'{X.shape} - Number of examples, size of an input into the neural net (block size number of ints)')\n",
    "print(f'type of data of X/Y: {X.dtype}')\n",
    "print(f'{Y.shape} - number of labels')\n",
    "\n",
    "\n",
    "print(X[:2])# input examples\n",
    "print(Y[:2]) # the labels for the input examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C embedding space matrix:\n",
      "tensor([[ 1.4771,  0.6115],\n",
      "        [ 0.0312, -0.6685],\n",
      "        [ 1.0566, -0.1957]])\n",
      "tensor([-0.3975,  0.6407])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.3975,  0.6407])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the Embedding lookup table 'C' (from paper at https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "\n",
    "NUM_CHARS = len(chars)+1 # add special char '.'\n",
    "\n",
    "# embed our possible characters from the training set in a lower dimensional space\n",
    "\n",
    "# for example, start with putting all into a 2 dimensional space\n",
    "# The C embedding space is initialized randomly to start\n",
    "embeddings_dims = 2\n",
    "C = torch.randn((NUM_CHARS, embeddings_dims)) # ex: NUM CHARS rows and 2 cols - each character in the training set has a 2-dimensional embedding (i.e. a character has a coordinate of an x and y in the space...)\n",
    "print('C embedding space matrix:')\n",
    "print(C[:3])\n",
    "\n",
    "### Example using C embedding space as a lookup for a character (i.e. the char repr. by 5)\n",
    "# we could just index into C:\n",
    "print(C[5]) # ex: the embedding for the char repr. by integer 5\n",
    "# we could also one hot encode for the char int repr of 5 and multiply it by C (is the same result as indexing above)\n",
    "F.one_hot(torch.tensor(5), num_classes=NUM_CHARS).float() @ C\n",
    "\n",
    "# This means that besides thinking of C as a lookup table using the int repr. of the char, we can also think of it as a first layer of a neural net using the one hot encoded approach\n",
    "# this layer is a linear layer with no tanh or activation function and the weights matrix is C\n",
    "# so we one hot encode characters and feed them into a layer of a neural net\n",
    "\n",
    "# NOTE: for the examples here we just index in because it's faster and we skip the one hot encoded approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 5]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 5]])\n",
      "tensor([[[ 1.4771,  0.6115],\n",
      "         [ 1.4771,  0.6115],\n",
      "         [ 1.4771,  0.6115]],\n",
      "\n",
      "        [[ 1.4771,  0.6115],\n",
      "         [ 1.4771,  0.6115],\n",
      "         [-0.3975,  0.6407]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4771,  0.6115],\n",
       "         [ 1.4771,  0.6115],\n",
       "         [ 1.4771,  0.6115]],\n",
       "\n",
       "        [[ 1.4771,  0.6115],\n",
       "         [ 1.4771,  0.6115],\n",
       "         [-0.3975,  0.6407]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "############################################################################################################################################################ \n",
    "#  indexing into C with X gets coordinates (a row in C) for each one of the characters in a block and puts that all into one row in the resulting Matrix \n",
    "#\n",
    "#                       See torch_advanced_indexing.ipynb notebook for examples and playground\n",
    "# ###########################################################################################################################################################\n",
    "# print(f'C[X] = {C[X]}') # X is a list of blocks\n",
    "print(X[:2])\n",
    "C[[0,0,5]] # example - the second block of X is a list so we index into C with that to get coordinates for each char in the block (the index of C lines up with coordinates for the char int representation)\n",
    "\n",
    "XX = X[:2]\n",
    "print(XX)\n",
    "\n",
    "print(C[XX])\n",
    "\n",
    "C[torch.tensor([[0, 0, 0],\n",
    "        [0, 0, 5]])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrices of embeddings and Weights are not matrix multipliable (cols of embeddings matrix must match rows of Weights matrix):\n",
      "shape of embeddings matrix (emb): torch.Size([790, 3, 2])\n",
      "shape of Weights matrix: torch.Size([6, 100])\n",
      "torch.Size([790, 6])\n",
      "has len of 3: Unbind(emb, 1):\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4771,  0.6115,  1.4771,  0.6115,  1.4771,  0.6115],\n",
       "        [ 1.4771,  0.6115,  1.4771,  0.6115, -0.3975,  0.6407],\n",
       "        [ 1.4771,  0.6115, -0.3975,  0.6407,  0.4360,  0.8485],\n",
       "        ...,\n",
       "        [-2.4419,  2.1903, -0.2650, -0.2605, -0.9303, -0.6410],\n",
       "        [-0.2650, -0.2605, -0.9303, -0.6410,  0.0684, -1.6289],\n",
       "        [-0.9303, -0.6410,  0.0684, -1.6289,  0.0684, -1.6289]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The embeddings for the characters in blocks\n",
    "emb = C[X] # using torch advanced indexing as explored above\n",
    "\n",
    "########## Construct the hidden layer \"Hop Parameter\" (timestamp 18:40)\n",
    "\n",
    "# initialize weights randomly \n",
    "num_inputs = block_size * embeddings_dims # example 3 x 2 (number of chars in a block multiplied by the embedding dimension gives you the number of inputs to the layer)\n",
    "num_neurons = 100 # variable number of neurons in the layer - you can choose and experiment with different numbers\n",
    "W1 = torch.randn((num_inputs, num_neurons)) # initialize random weights\n",
    "\n",
    "# initialize biases randomly as well\n",
    "b1 = torch.randn(num_neurons) # one for each neuron in the layer\n",
    "\n",
    "\n",
    "# The goal is to multiply the inputs by the weights and add the biases (as per normal in a NN), but the shapes of the embeddings matrix (emb) is not multipliable by the shape of W1 for matrix multiplication\n",
    "print('The matrices of embeddings and Weights are not matrix multipliable (cols of embeddings matrix must match rows of Weights matrix):')\n",
    "print(f'shape of embeddings matrix (emb): {emb.shape}')\n",
    "print(f'shape of Weights matrix: {W1.shape}')\n",
    "\n",
    "# we need to transform the embeddings matrix so that it's last dimension is equal to the first dimension of the Weights matrix: N1xM1 -> M1xN2\n",
    "  # can use torch.cat to concatenate the 3 embeddings in a row of the emb matrix (which are 3x2 sets, so 6 when concatenated)\n",
    "  # NOTE: should use views instead, though (see later cells)\n",
    "\n",
    "# this is the embeddings for the first character in each block of the trainingset\n",
    "first_char_emb = emb[:,0,:] # get all top lvl rows (blocks), get first embedding in each of those rows, get all elements in that embedding (the xy vector)\n",
    "second_char_emb = emb[:,1,:] # 2nd char in each block embeddings\n",
    "third_char_emb = emb[:,2,:] # third char in each block embeddings\n",
    "\n",
    "# torch.cat concatenates along the 2nd dimension (index 1 - second argument) to concatentate the embeddings for each character into 6 values (2 coords for each char)\n",
    "print(torch.cat([first_char_emb, second_char_emb, third_char_emb], 1).shape) # should have a cols number of 6 to make it matrix multipliable by the weights\n",
    "\n",
    "\n",
    "### NOTE: using cat and unbind is an inefficient way of accomplishing this and we should use tensor views instead (see below cells)\n",
    "# cat has to create a new tensor so new memory is used and needed where view does not (it accesses properties on the existing tensor)\n",
    "\n",
    "# to make the code more dynamic in case the block size changes, you can use torch.unbind()\n",
    "#print(torch.unbind(emb, 1))\n",
    "# print(torch.cat(torch.unbind(emb, 1), 1).shape) # same shape as previous torch.cat\n",
    "### See 23:25 for more explanation\n",
    "unb = torch.unbind(emb, 1)\n",
    "print('has len of 3: Unbind(emb, 1):')\n",
    "print(len(unb))\n",
    "torch.cat(torch.unbind(emb, 1), 1) # same thing as the previous torch.cat call except more dynamic based on block size (??)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4],\n",
      "         [5, 6]]])\n",
      "\n",
      "\n",
      "tensor([[1, 2]])\n",
      "test2:  tensor([[-1.2044, -0.4219,  0.2054, -1.3889, -0.6689],\n",
      "        [ 0.5901,  0.9371,  0.6702,  0.4017,  0.9293]])\n",
      "test3:  tensor([[-2.0624, -0.1915, -0.5579,  0.7462, -0.4422]])\n"
     ]
    }
   ],
   "source": [
    "test = torch.tensor([[[1,  2],\n",
    "         [3,  4],\n",
    "         [5,  6]]])\n",
    "\n",
    "print(test)\n",
    "print('\\n')\n",
    "print(test[:,0,:])\n",
    "# : = everything of the first dimension (all top level rows)\n",
    "# :,1 = all top rows and then get index 1 (2nd el) in each of the rows\n",
    "test[:,1]\n",
    "\n",
    "test2 = torch.randn(2,5)\n",
    "test2.shape\n",
    "print('test2: ', test2)\n",
    "\n",
    "test3 = torch.randn(1,5)\n",
    "test3.shape\n",
    "print('test3: ',test3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single vector of 18 before view:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
      "torch.Size([790, 3, 2])\n",
      "h Shape:  torch.Size([790, 100])\n",
      "tensor([[-0.9715, -0.9957, -0.9990,  ...,  0.9912, -0.8431,  0.5019],\n",
      "        [ 0.6854, -0.9968, -0.7634,  ..., -0.9470, -0.9959,  0.9388],\n",
      "        [ 0.9979, -0.9220, -0.9910,  ..., -0.9506,  0.8473,  0.4729],\n",
      "        ...,\n",
      "        [ 0.9996,  0.5055, -0.3891,  ..., -0.8670, -1.0000,  1.0000],\n",
      "        [ 0.9987,  0.9996, -0.2071,  ..., -0.8807, -0.9720,  0.0201],\n",
      "        [-0.6256,  0.9991, -0.3339,  ..., -0.0425, -0.9890, -0.4152]])\n",
      "torch.Size([790, 100])\n"
     ]
    }
   ],
   "source": [
    "# There is a better way than using unbind:\n",
    "\n",
    "# timestamp 23:30\n",
    "\n",
    "# array of els 0-17\n",
    "a = torch.arange(18)\n",
    "print('single vector of 18 before view: ', a)\n",
    "# print('shape: ', a.shape) # single vector of 18 numbers\n",
    "\n",
    "# You can represent the above as different size dimensional tensors\n",
    "# Use a view which is very efficient:\n",
    "a.view(2, 9) # change the single 18 sized vector to a 2 by 9 tensor\n",
    "a.view(3,3,2) # or change it to a 3x3x2 tensor (The total number of elements must multiply to be the same as 18)\n",
    "\n",
    "print(emb.shape) # 26x3x2\n",
    "emb.view(USE_THIS,6) # 26 and 3x2 (6) = 26, 6 - we can reshape the embeddings matrix with the view\n",
    "\n",
    "# print(emb.view(26,6) == torch.cat(torch.unbind(emb, 1), 1)) # using view is equivalent to the less efficient method with unbind and cat\n",
    "\n",
    "# This now gives us the hidden states we're after (see 28:16 timestamp - hidden layer of activations?)\n",
    "# Use shape[0] to not hard code, or You can also have pytorch derive the size for the view automatically using -1:     \n",
    "#    emb.view(-1, 6)\n",
    "h = emb.view(emb.shape[0],6) @ W1 + b1 # multiply embeddings vals by the weights and add the bias\n",
    "# NOTE on the bias: make sure the dimensions match up for broadcasting always - see timestamp 28:27\n",
    "print('h Shape: ', h.shape) # this is the 100 dimensional activations for our 27 examples\n",
    "\n",
    "# use tanh to make the activations between -1 and 1 (squashing function)\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "print(h)\n",
    "print(h.shape) # 26x100 - hidden layer of activations shown at timestamp 28:16 for every one of the 27 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final layer of the neural net (29:21 timetstamp)\n",
    "\n",
    "# initialize weights and biases\n",
    "W2 = torch.randn(100, USE_THIS) # 100 inputs and 27 possible characters that can come next\n",
    "b2 = torch.randn(USE_THIS) # the biases need to be 27 as well (match the columns dimension of weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape:  torch.Size([790, 790])\n"
     ]
    }
   ],
   "source": [
    "logits = h @ W2 + b2\n",
    "\n",
    "print('logits shape: ', logits.shape)\n",
    "# logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts shape:  torch.Size([790, 790])\n",
      "prob shape:  torch.Size([790, 790])\n",
      "every row of prob now sums to 1:\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# exponentiate the logits to get the fake counts\n",
    "counts = logits.exp()\n",
    "print('counts shape: ', counts.shape)\n",
    "\n",
    "# normalize them into a probability\n",
    "prob = counts / counts.sum(1, keepdim=True) # sum along the first dimension (the row)\n",
    "print('prob shape: ', prob.shape)\n",
    "print('every row of prob now sums to 1:')\n",
    "print(prob[0].sum()) # sum up everything along the first dimension (by rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5, 13, 13,  1,  0, 12,  9,  1, 13,  0, 15, 12,  9, 22,  9,  1,  0, 14,\n",
      "        15,  1,  8,  0,  1, 22,  1,  0,  5, 20,  8,  1, 14,  0, 19, 15, 16,  8,\n",
      "         9,  1,  0, 13,  1, 19, 15, 14,  0, 13,  1, 20, 20,  8,  5, 23,  0, 13,\n",
      "        15,  4,  9, 14,  5,  0,  9, 19,  1,  2,  5, 12, 12,  1,  0, 23,  9, 12,\n",
      "        12,  9,  1, 13,  0, 13,  9,  1,  0, 10,  1, 13,  5, 19,  0,  3,  8,  1,\n",
      "        18, 12, 15, 20, 20,  5,  0,  2,  5, 14, 10,  1, 13,  9, 14,  0,  1, 13,\n",
      "         5, 12,  9,  1,  0, 12, 21,  3,  1, 19,  0,  8,  1, 18, 16,  5, 18,  0,\n",
      "         8,  5, 14, 18, 25,  0,  5, 22,  5, 12, 25, 14,  0,  1, 12,  5, 24,  1,\n",
      "        14,  4,  5, 18,  0,  1,  2,  9,  7,  1,  9, 12,  0, 13,  9,  3,  8,  1,\n",
      "         5, 12,  0,  5, 13,  9, 12, 25,  0,  4,  1, 14,  9,  5, 12,  0,  5, 12,\n",
      "         9, 26,  1,  2,  5, 20,  8,  0, 10,  1,  3, 15,  2,  0, 13,  9, 12,  1,\n",
      "         0, 12, 15,  7,  1, 14,  0,  5, 12, 12,  1,  0, 10,  1,  3, 11, 19, 15,\n",
      "        14,  0,  1, 22,  5, 18, 25,  0, 19,  5,  2,  1, 19, 20,  9,  1, 14,  0,\n",
      "        19, 15,  6,  9,  1,  0, 10,  1,  3, 11,  0,  3,  1, 13,  9, 12,  1,  0,\n",
      "         1,  9,  4,  5, 14,  0,  1, 18,  9,  1,  0, 15, 23,  5, 14,  0, 19,  3,\n",
      "         1, 18, 12,  5, 20, 20,  0, 19,  1, 13, 21,  5, 12,  0, 22,  9,  3, 20,\n",
      "        15, 18,  9,  1,  0, 13,  1,  4,  9, 19, 15, 14,  0, 10, 15, 19,  5, 16,\n",
      "         8,  0, 12, 21, 14,  1,  0, 12,  5, 22,  9,  0,  7, 18,  1,  3,  5,  0,\n",
      "        13,  1, 20,  5, 15,  0,  3,  8, 12, 15,  5,  0,  4,  1, 22,  9,  4,  0,\n",
      "        16,  5, 14,  5, 12, 15, 16,  5,  0, 10, 15,  8, 14,  0, 12,  1, 25, 12,\n",
      "         1,  0, 23, 25,  1, 20, 20,  0, 18,  9, 12,  5, 25,  0, 15, 12,  9, 22,\n",
      "         5, 18,  0, 26, 15,  5, 25,  0, 10,  1, 25,  4,  5, 14,  0, 14, 15, 18,\n",
      "         1,  0,  4, 25, 12,  1, 14,  0, 12,  9, 12, 25,  0, 12, 21, 11,  5,  0,\n",
      "         5, 12,  5,  1, 14, 15, 18,  0,  7,  1,  2, 18,  9,  5, 12,  0,  8,  1,\n",
      "        14, 14,  1,  8,  0,  1, 14, 20,  8, 15, 14, 25,  0, 12,  9, 12, 12,  9,\n",
      "         1, 14,  0,  9, 19,  1,  1,  3,  0,  1,  4,  4,  9, 19, 15, 14,  0,  7,\n",
      "        18,  1, 25, 19, 15, 14,  0,  1, 21,  2, 18,  5, 25,  0, 10, 21, 12,  9,\n",
      "         1, 14,  0,  5, 12, 12,  9,  5,  0,  3,  8, 18,  9, 19, 20, 15, 16,  8,\n",
      "         5, 18,  0, 19, 20,  5, 12, 12,  1,  0, 10, 15, 19,  8, 21,  1,  0, 14,\n",
      "         1, 20,  1, 12,  9,  5,  0,  1, 14,  4, 18,  5, 23,  0, 26, 15,  5,  0,\n",
      "        12,  9, 14,  3, 15, 12, 14,  0, 12,  5,  1,  8,  0, 18, 25,  1, 14,  0,\n",
      "         8,  1, 26,  5, 12,  0, 14,  1, 20,  8,  1, 14,  0, 22,  9, 15, 12,  5,\n",
      "        20,  0,  1,  4,  1, 13,  0,  1, 21, 18, 15, 18,  1,  0, 12,  5, 15,  0,\n",
      "        19,  1, 22,  1, 14, 14,  1,  8,  0, 10,  1, 24, 15, 14,  0,  1, 21,  4,\n",
      "        18,  5, 25,  0,  9, 19,  1,  9,  1,  8,  0,  2, 18, 15, 15, 11, 12, 25,\n",
      "        14,  0,  5, 12,  9,  0,  2,  5, 12, 12,  1,  0,  1,  1, 18, 15, 14,  0,\n",
      "         3, 12,  1,  9, 18,  5,  0,  3,  1, 18, 19, 15, 14,  0, 19, 11, 25, 12,\n",
      "         1, 18,  0,  3,  8,  1, 18, 12,  5, 19,  0, 12, 21,  3, 25,  0, 20,  8,\n",
      "        15, 13,  1, 19,  0, 16,  1,  9, 19, 12,  5, 25,  0,  3,  1, 12,  5,  2,\n",
      "         0,  5, 22,  5, 18, 12, 25,  0, 10, 15, 19,  9,  1,  8,  0,  1, 14, 14,\n",
      "         1,  0,  3,  8, 18,  9, 19, 20,  9,  1, 14,  0,  3,  1, 18, 15, 12,  9,\n",
      "        14,  5,  0,  8, 21, 14, 20,  5, 18,  0, 14, 15, 22,  1,  0, 12,  1, 14,\n",
      "         4, 15, 14,  0,  7,  5, 14,  5, 19,  9, 19,  0, 10, 15, 14,  1, 20,  8,\n",
      "         1, 14,  0,  5, 13,  9, 12,  9,  1,  0, 17, 21,  9, 14, 14,  0])\n",
      "torch.Size([790])\n",
      "torch.Size([790, 790])\n",
      "torch.Size([790])\n",
      "probs:  tensor([3.8018e-13, 3.0901e-13, 2.3600e-13, 2.8401e-11, 2.2296e-09, 1.4709e-11,\n",
      "        3.0565e-14, 3.7746e-12, 5.3325e-04, 6.7920e-12, 1.3040e-23, 3.0829e-15,\n",
      "        5.5740e-09, 4.6279e-08, 9.8363e-10, 1.0820e-13, 5.7714e-10, 6.4997e-04,\n",
      "        1.0831e-13, 1.3721e-12, 3.4072e-09, 5.2289e-17, 6.6727e-11, 9.2960e-14,\n",
      "        3.6027e-14, 1.5208e-08, 3.8018e-13, 7.6384e-17, 1.3464e-18, 1.7691e-16,\n",
      "        1.8428e-23, 1.1134e-14, 9.7270e-09, 3.0579e-21, 9.0361e-11, 6.3773e-21,\n",
      "        2.1417e-17, 9.4504e-13, 7.0327e-15, 1.4968e-12, 3.5786e-09, 1.1639e-09,\n",
      "        1.0702e-18, 2.6624e-08, 5.5738e-14, 1.4968e-12, 3.5786e-09, 2.3876e-20,\n",
      "        3.3794e-17, 1.3030e-05, 3.2585e-18, 1.2410e-15, 4.0335e-12, 1.4968e-12,\n",
      "        7.3311e-22, 6.4115e-13, 6.0907e-10, 9.2870e-16, 1.0325e-15, 1.9802e-14,\n",
      "        2.7083e-12, 2.7939e-10, 3.2815e-11, 2.9298e-16, 2.9977e-16, 9.6497e-11,\n",
      "        2.1224e-12, 5.6937e-16, 9.7603e-15, 9.0901e-10, 6.0000e-14, 7.8660e-15,\n",
      "        4.7589e-14, 3.1793e-18, 1.6318e-07, 5.3325e-04, 6.7920e-12, 1.4968e-12,\n",
      "        6.6873e-12, 7.9811e-09, 7.2619e-13, 1.2028e-08, 1.5290e-09, 1.8099e-07,\n",
      "        1.2888e-12, 1.3368e-05, 2.5629e-13, 1.3617e-08, 3.0078e-13, 1.8802e-11,\n",
      "        1.1269e-15, 2.8874e-11, 6.5526e-15, 3.3256e-16, 1.2966e-13, 3.1373e-11,\n",
      "        9.2636e-12, 1.3724e-19, 4.3461e-14, 1.6751e-09, 7.6959e-13, 3.3118e-15,\n",
      "        1.6953e-07, 3.4445e-09, 1.2133e-10, 1.6632e-14, 6.6727e-11, 7.3526e-09,\n",
      "        2.2777e-10, 1.6468e-08, 3.4042e-13, 3.7783e-17, 1.2668e-12, 1.4709e-11,\n",
      "        3.2680e-11, 3.9315e-12, 2.8113e-08, 8.0056e-12, 3.7777e-13, 1.6226e-14,\n",
      "        2.1431e-12, 4.4627e-16, 3.8646e-15, 1.1609e-06, 2.5477e-07, 3.5411e-12,\n",
      "        1.6226e-14, 2.5618e-19, 3.4415e-18, 1.0758e-07, 1.3399e-16, 7.6811e-13,\n",
      "        3.8018e-13, 4.9394e-13, 1.7857e-16, 1.4033e-05, 2.0720e-07, 2.1687e-08,\n",
      "        2.6380e-10, 6.6727e-11, 2.2875e-13, 5.7623e-13, 2.6977e-09, 2.1477e-04,\n",
      "        5.3217e-11, 2.4344e-12, 1.3274e-15, 6.8218e-08, 4.8617e-12, 6.6727e-11,\n",
      "        1.4144e-20, 4.2423e-09, 1.9850e-12, 5.9477e-08, 3.2954e-09, 5.0062e-13,\n",
      "        7.1144e-13, 1.4968e-12, 6.6873e-12, 6.5259e-13, 1.5029e-21, 2.0781e-15,\n",
      "        2.4145e-10, 1.2328e-11, 5.9356e-13, 3.8018e-13, 3.0901e-13, 3.3829e-05,\n",
      "        6.1123e-11, 9.3337e-08, 1.1180e-12, 4.7466e-17, 1.3070e-10, 7.2444e-16,\n",
      "        2.1247e-16, 4.8902e-12, 4.1638e-14, 4.5341e-13, 3.8018e-13, 1.9075e-14,\n",
      "        2.6745e-11, 3.4046e-09, 1.7202e-10, 3.9287e-17, 2.7629e-07, 3.0616e-13,\n",
      "        1.1590e-12, 3.7883e-13, 1.2028e-08, 1.5290e-09, 4.2298e-14, 6.0436e-18,\n",
      "        3.6607e-12, 7.7572e-12, 1.4968e-12, 6.6873e-12, 1.1833e-14, 2.1883e-13,\n",
      "        1.9223e-14, 1.4709e-11, 6.0562e-19, 5.6276e-10, 2.3027e-06, 2.9602e-18,\n",
      "        7.1992e-14, 3.8018e-13, 1.9075e-14, 1.9764e-12, 5.6937e-16, 9.7603e-15,\n",
      "        1.2028e-08, 1.5290e-09, 4.2298e-14, 1.4409e-14, 6.9325e-06, 1.5519e-20,\n",
      "        2.5227e-07, 5.5738e-14, 6.6727e-11, 9.2960e-14, 7.2440e-15, 7.8477e-06,\n",
      "        3.2841e-12, 2.4386e-09, 9.7270e-09, 5.3765e-14, 2.7073e-19, 5.3080e-12,\n",
      "        8.1506e-06, 7.3539e-13, 2.3370e-11, 2.2196e-17, 6.6166e-20, 4.3838e-14,\n",
      "        9.7270e-09, 3.0579e-21, 2.7053e-16, 3.1690e-08, 1.1241e-16, 8.0163e-15,\n",
      "        1.2028e-08, 1.5290e-09, 4.2298e-14, 1.4409e-14, 2.4110e-11, 1.3617e-08,\n",
      "        1.5316e-09, 7.8401e-08, 5.5088e-08, 4.6711e-12, 2.1883e-13, 1.9223e-14,\n",
      "        6.6727e-11, 4.5965e-12, 2.1280e-09, 6.9136e-13, 2.3155e-09, 5.2921e-13,\n",
      "        6.6727e-11, 1.9447e-13, 1.0056e-13, 1.3324e-10, 3.1578e-10, 1.3040e-23,\n",
      "        4.4420e-13, 3.0288e-12, 2.5521e-12, 6.4971e-13, 9.7270e-09, 2.9395e-09,\n",
      "        4.7739e-08, 6.7383e-13, 1.7088e-15, 8.5552e-08, 6.8615e-13, 1.3451e-14,\n",
      "        4.0695e-11, 9.7270e-09, 1.3993e-08, 3.4596e-08, 1.1881e-17, 1.1170e-11,\n",
      "        2.1633e-12, 2.4400e-13, 1.7179e-10, 3.2043e-14, 2.5811e-13, 7.6646e-10,\n",
      "        2.5231e-15, 1.4501e-16, 5.9730e-13, 9.1413e-14, 3.1578e-10, 1.4968e-12,\n",
      "        3.5786e-09, 5.0317e-14, 7.3552e-13, 2.7379e-09, 2.2417e-14, 1.0863e-06,\n",
      "        5.5738e-14, 1.2028e-08, 1.2695e-21, 7.0161e-09, 1.0881e-13, 4.5583e-14,\n",
      "        4.4468e-18, 2.7220e-14, 1.4709e-11, 3.2680e-11, 2.3009e-14, 4.6641e-09,\n",
      "        1.1025e-16, 1.4709e-11, 4.9983e-11, 3.9292e-13, 9.8042e-08, 6.2115e-13,\n",
      "        3.9906e-10, 2.5542e-15, 3.3450e-10, 2.1017e-15, 3.9841e-10, 1.1067e-10,\n",
      "        1.4968e-12, 3.5786e-09, 2.3876e-20, 9.5571e-14, 1.7364e-11, 1.6587e-08,\n",
      "        1.3617e-08, 3.0078e-13, 1.7037e-17, 1.2722e-17, 8.4722e-12, 1.9276e-15,\n",
      "        4.7466e-17, 1.3070e-10, 1.5712e-12, 7.4100e-15, 2.1187e-11, 3.8345e-11,\n",
      "        1.4592e-13, 8.8359e-18, 2.9517e-12, 1.5328e-16, 2.2181e-09, 4.9613e-16,\n",
      "        4.9140e-14, 2.7282e-12, 4.6282e-10, 1.2028e-08, 1.2695e-21, 4.2971e-16,\n",
      "        2.5693e-24, 1.2591e-12, 1.4709e-11, 3.1604e-10, 4.3563e-14, 3.6612e-17,\n",
      "        1.2155e-09, 1.3069e-12, 9.0901e-10, 1.8568e-19, 1.4750e-10, 2.3249e-18,\n",
      "        7.6130e-17, 1.1688e-09, 2.3731e-13, 3.0423e-14, 1.0052e-14, 1.5333e-08,\n",
      "        8.1288e-18, 6.9959e-13, 1.3040e-23, 3.0829e-15, 5.5740e-09, 4.6279e-08,\n",
      "        3.0588e-14, 6.1941e-06, 5.5511e-12, 5.1433e-14, 1.6639e-20, 3.7706e-17,\n",
      "        1.0218e-17, 1.1781e-10, 1.2028e-08, 1.5290e-09, 4.9107e-12, 9.3794e-15,\n",
      "        8.8884e-09, 1.5754e-11, 5.2921e-13, 6.4997e-04, 1.0831e-13, 1.5779e-15,\n",
      "        8.2376e-11, 6.7450e-12, 4.7466e-17, 2.1884e-17, 1.7898e-13, 5.2832e-15,\n",
      "        3.8637e-14, 1.3399e-15, 1.4709e-11, 3.0565e-14, 2.5470e-16, 4.1304e-14,\n",
      "        1.1180e-12, 1.4709e-11, 3.2680e-11, 8.6604e-17, 2.1939e-13, 5.1440e-10,\n",
      "        3.8018e-13, 1.9075e-14, 5.3279e-11, 1.7026e-17, 8.1039e-13, 1.1302e-08,\n",
      "        2.3693e-11, 1.4918e-13, 3.9906e-10, 7.5844e-11, 2.5181e-11, 2.2063e-08,\n",
      "        1.5904e-15, 1.8842e-12, 7.9016e-06, 4.5341e-13, 1.6226e-14, 2.1431e-12,\n",
      "        8.4614e-23, 1.8735e-15, 9.1825e-15, 6.4659e-08, 5.1335e-16, 6.6727e-11,\n",
      "        7.5872e-07, 4.8296e-16, 2.8959e-08, 1.2572e-07, 5.1611e-21, 1.0181e-11,\n",
      "        2.2148e-13, 1.4709e-11, 3.0565e-14, 2.5470e-16, 2.9979e-16, 3.1793e-18,\n",
      "        1.6318e-07, 6.9250e-19, 4.3838e-14, 2.7083e-12, 2.7939e-10, 3.2815e-11,\n",
      "        3.6520e-15, 4.0594e-15, 2.0948e-12, 6.6727e-11, 4.0768e-15, 1.1939e-12,\n",
      "        4.7709e-09, 3.9966e-09, 2.2417e-14, 1.0863e-06, 5.5738e-14, 3.9906e-10,\n",
      "        2.5542e-15, 3.3450e-10, 2.9489e-08, 9.8136e-06, 1.5041e-19, 2.4653e-07,\n",
      "        5.5738e-14, 6.6727e-11, 8.0115e-12, 1.0943e-12, 4.4749e-09, 7.0823e-14,\n",
      "        2.4551e-15, 1.4448e-12, 1.2028e-08, 2.1196e-13, 3.3714e-13, 4.8849e-14,\n",
      "        2.2874e-13, 6.9250e-19, 4.3838e-14, 3.8018e-13, 1.9075e-14, 1.9764e-12,\n",
      "        2.9011e-18, 1.6183e-12, 1.2921e-15, 1.3617e-08, 3.0078e-13, 2.4552e-16,\n",
      "        9.0861e-12, 5.8829e-14, 1.1224e-10, 4.3052e-14, 7.0327e-16, 1.5636e-18,\n",
      "        9.1125e-16, 5.2689e-12, 1.5846e-12, 9.7270e-09, 4.7722e-15, 1.0194e-12,\n",
      "        1.1228e-13, 1.6273e-11, 5.6937e-16, 9.7603e-15, 1.2028e-08, 1.2695e-21,\n",
      "        7.0161e-09, 7.0084e-23, 1.4881e-11, 2.2547e-11, 6.4066e-15, 6.4997e-04,\n",
      "        7.8870e-13, 1.0995e-11, 3.7227e-08, 7.1778e-17, 2.1436e-14, 1.5191e-13,\n",
      "        1.2921e-15, 6.6727e-11, 7.5872e-07, 2.6735e-10, 1.9036e-12, 7.2346e-07,\n",
      "        4.6769e-16, 2.4936e-11, 5.1433e-14, 1.6639e-20, 3.7706e-17, 2.9170e-15,\n",
      "        1.4709e-11, 3.0565e-14, 1.1856e-16, 3.3066e-19, 5.3668e-16, 2.1728e-13,\n",
      "        3.2654e-23, 1.4304e-17, 1.4709e-11, 4.9983e-11, 1.7408e-13, 1.7209e-09,\n",
      "        5.2323e-16, 2.3731e-13, 1.5446e-19, 8.2067e-11, 3.4424e-14, 6.0531e-14,\n",
      "        1.6226e-14, 2.1431e-12, 2.3850e-09, 8.9219e-08, 5.0683e-12, 7.8865e-11,\n",
      "        6.4997e-04, 7.8870e-13, 1.0995e-11, 8.2273e-06, 1.3402e-13, 1.8428e-23,\n",
      "        1.1134e-14, 1.7179e-10, 3.2043e-14, 4.3609e-06, 3.3008e-13, 3.4259e-06,\n",
      "        1.0972e-12, 1.8425e-08, 6.6727e-11, 4.0768e-15, 1.5646e-12, 4.1747e-05,\n",
      "        2.1930e-11, 6.6727e-11, 8.0115e-12, 1.2880e-14, 6.5514e-09, 5.1397e-11,\n",
      "        1.5330e-17, 6.7450e-12, 1.4709e-11, 4.9983e-11, 2.0333e-17, 4.4962e-10,\n",
      "        9.7270e-09, 1.3993e-08, 5.6440e-15, 3.7076e-18, 1.5537e-14, 1.1862e-19,\n",
      "        9.1825e-15, 6.4659e-08, 5.1335e-16, 1.2028e-08, 1.5290e-09, 6.0450e-12,\n",
      "        1.3153e-16, 3.3824e-11, 4.0897e-13, 6.6727e-11, 8.0115e-12, 5.2542e-09,\n",
      "        1.6723e-10, 3.0404e-12, 2.4702e-08, 1.4448e-12, 2.7083e-12, 2.7939e-10,\n",
      "        3.2815e-11, 1.6367e-12, 1.2416e-16, 3.5048e-10, 5.1804e-17, 1.3724e-19,\n",
      "        3.4468e-13, 6.6992e-16, 5.5103e-04, 8.3210e-14, 1.8719e-07, 4.4369e-08,\n",
      "        2.0721e-08, 2.6380e-10, 3.8018e-13, 1.9075e-14, 2.6745e-11, 2.4705e-11,\n",
      "        1.3724e-19, 4.3461e-14, 2.2986e-13, 2.1224e-12, 5.6937e-16, 9.7603e-15,\n",
      "        6.6727e-11, 2.8395e-10, 3.3072e-15, 5.5715e-09, 1.1854e-17, 1.8138e-12,\n",
      "        1.3617e-08, 8.2105e-13, 5.2999e-08, 7.8143e-15, 2.3997e-10, 4.2152e-15,\n",
      "        9.6361e-11, 1.3617e-08, 1.5316e-09, 5.0826e-15, 2.5326e-11, 3.1593e-20,\n",
      "        1.8842e-07, 5.5738e-14, 9.7270e-09, 7.1916e-11, 1.9090e-19, 6.0573e-10,\n",
      "        2.7227e-15, 4.7224e-17, 1.0063e-13, 1.3617e-08, 3.0078e-13, 1.8802e-11,\n",
      "        1.1269e-15, 2.8874e-11, 8.5552e-08, 1.1485e-05, 4.9861e-12, 1.4709e-11,\n",
      "        3.2680e-11, 3.9315e-12, 3.7044e-18, 5.5677e-11, 2.7791e-16, 7.1206e-13,\n",
      "        3.6767e-08, 4.9915e-11, 4.6704e-11, 2.2556e-14, 5.3200e-14, 1.4592e-13,\n",
      "        4.2612e-10, 2.0341e-10, 2.5830e-07, 2.2633e-10, 1.9447e-12, 2.8547e-14,\n",
      "        6.9959e-13, 1.3617e-08, 1.5316e-09, 7.2395e-14, 2.0778e-13, 5.7163e-12,\n",
      "        2.9495e-08, 3.8018e-13, 4.9394e-13, 1.7857e-16, 5.0923e-05, 2.4081e-07,\n",
      "        8.7579e-05, 1.6210e-09, 1.2028e-08, 1.2695e-21, 7.0161e-09, 7.5868e-08,\n",
      "        1.0485e-15, 7.5769e-20, 5.1804e-17, 6.6727e-11, 7.5872e-07, 1.2008e-19,\n",
      "        9.1825e-15, 5.9302e-14, 1.3617e-08, 3.0078e-13, 2.4552e-16, 9.0861e-12,\n",
      "        5.8829e-14, 1.1224e-10, 1.7075e-11, 2.2196e-17, 6.6166e-20, 4.3838e-14,\n",
      "        1.3617e-08, 1.5316e-09, 5.0826e-15, 3.0947e-14, 9.7318e-13, 7.9500e-12,\n",
      "        1.7071e-13, 5.1299e-14, 1.9802e-14, 1.6226e-14, 2.3526e-12, 1.1267e-19,\n",
      "        7.0489e-13, 4.4862e-06, 1.5555e-07, 2.0256e-12, 6.4997e-04, 1.0831e-13,\n",
      "        6.7651e-11, 9.5063e-12, 1.8401e-11, 1.4709e-11, 3.1604e-10, 9.5671e-16,\n",
      "        1.1060e-11, 2.1016e-11, 1.3785e-13, 2.9419e-14, 3.9906e-10, 4.1344e-19,\n",
      "        1.0899e-16, 2.3240e-13, 3.5856e-07, 3.1891e-09, 6.8851e-11, 4.9734e-13,\n",
      "        1.2028e-08, 1.2695e-21, 5.0510e-12, 8.8173e-20, 1.5169e-11, 8.2273e-06,\n",
      "        1.3402e-13, 1.8428e-23, 1.1134e-14, 3.8018e-13, 3.0901e-13, 3.3829e-05,\n",
      "        6.1123e-11, 5.5745e-12, 1.3904e-14, 1.2668e-12, 1.8037e-14, 6.9791e-14,\n",
      "        3.5128e-07, 4.6617e-17, 2.3615e-23, 1.3944e-14])\n"
     ]
    }
   ],
   "source": [
    "# The next layer comes from Y which created during data separation - the next character in the sequence we want to predict\n",
    "#   See timestamp 30:40\n",
    "print(Y)\n",
    "print(Y.shape)\n",
    "print(prob.shape)\n",
    "\n",
    "print(torch.arange(Y.shape[0]).shape)\n",
    "\n",
    "# index into each row of probabilities and pluck out the probability assigned to the correct character\n",
    "probabilities = prob[torch.arange(Y.shape[0]), Y] # before training a lot of these will be very close to 0\n",
    "# After training ideally all the numbers should be 1 which means we're correctly predicting the next character\n",
    "print('probs: ', probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(27.5185)\n"
     ]
    }
   ],
   "source": [
    "# Get the negative log likelihood\n",
    "# take the log of the probability and get the average log probability and take the negative of it\n",
    "loss = -prob[torch.arange(Y.shape[0]), Y].log().mean()\n",
    "print('loss: ', loss) # this is the loss we want to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([790, 3]), torch.Size([790]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############# REWRITTEN FROM TIMESTAMP 32:25 ##################\n",
    "\n",
    "# Dataset\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # to make deterministic and reproducable\n",
    "# parameters:\n",
    "C = torch.randn((USE_THIS, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, USE_THIS), generator=g)\n",
    "b2 = torch.randn(USE_THIS, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  82070\n"
     ]
    }
   ],
   "source": [
    "print('Number of parameters: ', sum(p.nelement() for p in parameters)) # total parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.423449516296387\n",
      "13.593306541442871\n",
      "16.120805740356445\n",
      "13.175605773925781\n",
      "10.533838272094727\n",
      "12.658477783203125\n",
      "12.15114688873291\n",
      "14.161653518676758\n",
      "13.379488945007324\n",
      "11.827338218688965\n",
      "12.270668029785156\n",
      "9.604423522949219\n",
      "9.870196342468262\n",
      "9.376617431640625\n",
      "9.859009742736816\n",
      "10.342354774475098\n",
      "7.551503658294678\n",
      "6.221015453338623\n",
      "10.087067604064941\n",
      "7.332768440246582\n",
      "9.642024040222168\n",
      "6.610126972198486\n",
      "11.33931827545166\n",
      "8.919302940368652\n",
      "9.880581855773926\n",
      "8.215899467468262\n",
      "9.752113342285156\n",
      "8.777703285217285\n",
      "8.369864463806152\n",
      "7.183315277099609\n",
      "7.9839067459106445\n",
      "7.251392841339111\n",
      "10.04757308959961\n",
      "9.06892204284668\n",
      "8.216794967651367\n",
      "5.529954433441162\n",
      "4.962522506713867\n",
      "9.235088348388672\n",
      "5.746991157531738\n",
      "5.8533148765563965\n",
      "6.736738681793213\n",
      "6.80381441116333\n",
      "5.472954750061035\n",
      "4.757626056671143\n",
      "4.711280345916748\n",
      "3.965932846069336\n",
      "6.654690742492676\n",
      "5.054947376251221\n",
      "4.748981475830078\n",
      "6.900294303894043\n",
      "6.73411750793457\n",
      "5.934909343719482\n",
      "4.919130802154541\n",
      "6.0589375495910645\n",
      "5.059152603149414\n",
      "4.840741157531738\n",
      "6.109846591949463\n",
      "4.114644527435303\n",
      "5.309147357940674\n",
      "5.410819053649902\n",
      "5.057456970214844\n",
      "3.9771082401275635\n",
      "4.185337066650391\n",
      "6.593626499176025\n",
      "3.6429507732391357\n",
      "5.153360366821289\n",
      "4.351973533630371\n",
      "4.904800891876221\n",
      "4.178693771362305\n",
      "5.945453643798828\n",
      "4.31384801864624\n",
      "4.453430652618408\n",
      "5.389851093292236\n",
      "2.8250749111175537\n",
      "4.310116767883301\n",
      "5.167873859405518\n",
      "4.025389671325684\n",
      "4.842247486114502\n",
      "5.766302585601807\n",
      "3.2136034965515137\n",
      "4.056366443634033\n",
      "4.528444290161133\n",
      "4.04937219619751\n",
      "4.8191022872924805\n",
      "3.5449109077453613\n",
      "3.1579227447509766\n",
      "3.4921185970306396\n",
      "3.2507882118225098\n",
      "3.752695083618164\n",
      "3.6549179553985596\n",
      "3.737967014312744\n",
      "3.376417636871338\n",
      "3.6481990814208984\n",
      "3.8103299140930176\n",
      "3.4084293842315674\n",
      "3.7623019218444824\n",
      "4.01104211807251\n",
      "4.337820053100586\n",
      "3.3013734817504883\n",
      "3.9551191329956055\n"
     ]
    }
   ],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True # needed to do with pytorch before starting gradient descent iterations\n",
    "\n",
    "for _ in range(100):\n",
    "    # contruct a mini-batch for efficiency:\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # integers we want to optimize in this iteration. See timestamp 43:38\n",
    "\n",
    "    # Forward Pass\n",
    "    emb = C[X[ix]] # (NUM_EXAMPLES, 3, 2). Only grab the rows of the mini-batch\n",
    "    # print(emb.shape)\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (26, 100)\n",
    "    logits = h @ W2 + b2 # (26, 26)\n",
    "    # This is classification:\n",
    "    # You would never do this manually in practice - it is very inefficient and see reasons around timestamp 36:00\n",
    "    # counts = logits.exp()\n",
    "    # prob = counts / counts.sum(1, keepdim=True)\n",
    "    # loss = -prob[torch.arange(emb.shape[0]), Y].log().mean()\n",
    "    loss = F.cross_entropy(logits, Y[ix]) # use builtin cross_entropy() instead of calc manually above - better and efficient. Also use the mini-batch by passing ix into indexing into Y - makes it MUCH FASTER\n",
    "    print(loss.item()) # expresses how well the neural network works with current set of parameters\n",
    "\n",
    "    # Backward Pass\n",
    "    for p in parameters:\n",
    "        p.grad = None # same as setting it to 0 in pytorch\n",
    "    # use .backward to populate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # update the parameters\n",
    "    LEARNING_RATE = 0.1\n",
    "    for p in parameters:\n",
    "        p.data += -LEARNING_RATE * p.grad # nudge the parameters by the learning rate\n",
    "\n",
    "\n",
    "\n",
    "# The loss should show as decreasing more and more with more iterations\n",
    "\n",
    "# NOTE: Overfitting is occurring with the above because you have so many parameters (3,378) for so few examples (26 or 32)\n",
    "# This makes the loss very easy to make low\n",
    "# See timestamp 40:40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "builtin cross entropy:  tensor(21.7302, grad_fn=<NllLossBackward0>)\n",
      "original loss:  tensor(21.7302, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# we can use built in function for classification to get the loss\n",
    "builtin = F.cross_entropy(logits, Y) \n",
    "print('builtin cross entropy: ', builtin)\n",
    "print('original loss: ', loss) # manual calc is the same as the builtin cross_entropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([175, 670, 387,  11, 541,   4, 699, 596, 381, 737, 323, 417, 443, 482,\n",
       "        599, 114, 318, 337, 116, 306, 293, 625, 783, 685, 375, 411, 523, 755,\n",
       "        379, 643, 457, 709])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexes into the dataset - random ints and there are 32 of them\n",
    "torch.randint(0, X.shape[0], (32,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
